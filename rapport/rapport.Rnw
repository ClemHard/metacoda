\documentclass[11pt,a4paper]{report}
\usepackage{geometry}
\geometry{hmargin=2cm,vmargin=2cm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage[french]{babel}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{float}
\usepackage{caption}
\usepackage{bbm}
\usepackage[singlespacing]{setspace}
\usepackage{float}
\usepackage{textcomp}
\usepackage{apptools}
\usepackage{tabularx}
\usepackage{makecell} 
\usepackage{blkarray}
\usepackage{calc}
\usepackage{multirow}
\usepackage{array}
\usepackage{bigdelim}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes}

\tikzstyle{decision} = [diamond, draw, text width=4.5em, 
                        text badly centered, node distance=2cm, 
                        inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, 
                     text centered, rounded corners, 
                     minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, circle, minimum height=3em]

\newcommand{\ER}{\ensuremath{\mathbb{R}}}
\newcommand{\EE}{\ensuremath{\mathbb{E}}}

\tikzset{
>=stealth',
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

<<setup, include=FALSE, echo=FALSE>>=
library(knitr)
library(kableExtra)
library(xtable)
library(doParallel)
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, fig.path = "figure/", cache.path = "cache/")
knitr::opts_knit$set(root.dir = "..")
@

\title{Exploration des méthodes d'analyse de données compositionnelles pour l'étude du lien entre microbiote intestinal et santé.}

\date{}

\begin{document}

<<chargement_source, echo=FALSE,  include=FALSE>>=
source("Rscript/read_metagenomic_data.R")
source("Rscript/coda.R")
source("Rscript/comparaison_clustering.R")
source("Rscript/graph.R")
source("Rscript/bootstrap.R")
source("Rscript/test_bootstrap.R")
source("Rscript/tree_phyloseq.R")
source("Rscript/apprentissage_supervise.R")
source("Rscript/fonction_presentation.R")
@


<<chargement RData, echo=FALSE, include=FALSE>>=
load("rapport/resultats.RData")
@


\maketitle

\newpage
\tableofcontents
\newpage
\section{Présentation}
Je réalise mon stage à l'Inra de Jouy en Josas dans l'unité "Mathématiques et Informatique Appliquées du Génome à l'Environnement" (MaIAGE).

\subsection{Unité de recherche}
L'unité de recherche MaIAGE regroupe des mathématiciens, des informaticiens, des bioinformaticiens et des biologistes autour de questions de biologie et agro-écologie, allant de l'échelle moléculaire à l'échelle du paysage en passant par l'étude de l'individu, de populations ou d'écosystèmes.

L'unité développe des méthodes mathématiques et informatiques originales de portée générique ou motivées par des problèmes biologiques précis. Elle s'implique aussi dans la mise à disposition de bases de données et de logiciels permettant aux biologistes d'utiliser les outils dans de bonnes conditions ou d'exploiter automatiquement la littérature scientifique.

L'inférence statistique et la modélisation dynamique sont des compétences fortes de l'unité, auxquelles s'ajoutent la bioinformatique, l'automatique et l'algorithmique. Les activités de recherche et d'ingénierie s'appuient également sur une forte implication dans les disciplines destinatrices : écologie, environnement, biologie moléculaire et biologie des systèmes.

L'unité MaIAGE est structurée en 4 équipes de recherche :

\begin{enumerate}
\item bioinformatique et statistique pour les données "omiques"(StatInfOmics)
\item biologie des Systèmes(BioSys)
\item modélisation dynamique et statistique pour les écosystèmes, l'épidémiologie et l'agronomie(Dynenvie)
\item  acquisition et formalisation de connaissances à partir de texte(Bibliome )
\end{enumerate}

J'ai réalisé mon stage au sein de l'équipe StatInfOmics.
\subsection{Equipe}
L'équipe StatInfOmics vise à développer et mettre en oeuvre des méthodes statistiques et bioinformatiques dédiées à l’analyse de données “omiques”. D’un point de vue biologique, les questions abordées concernent principalement l’annotation structurale et fonctionnelle des génomes, les régulations géniques, la dynamique évolutive des génomes, et la caractérisation d’écosystèmes microbiens en terme de diversité et de fonctions présentes ; une cible commune étant la relation entre génotype et phénotype. Une part de plus en plus importante de notre activité est relative à l’intégration de données “omiques” hétérogènes pour en extraire de l’information pertinente et aussi prédire des processus biologiques. D’un point de vue méthodologique, nos travaux sont essentiellement d’ordre statistique : estimation de distributions, inférence de modèles à variables latentes, prédiction de relations entre jeux de variables, segmentation, visualisation et classification, avec une attention particulière au cadre de la grande dimension qui caractérise la majorité des jeux de données “omiques” étudiés. Ces recherches s’appuient souvent sur une ingénierie bioinformatique très forte.


\newpage





\section{Contexte biologique et enjeux}
\subsection{Métagénomique}
La métagénomique est une méthode de séquencage et d'analyse du contenu génétique d'un échantillon issu d'un milieu complexe (tel que le microbiote intestinal). La métagénomique à l'inverse de la génomique séquence le génome de l'ensemble des espèces présentes dans un échantillon. Cette évolution a pu être réalisée grâce aux nouvelles méthodes de séquencage dit de séquencage haut débit ainsi qu'a la bioinformatique.\\
En bactériologie, le génome n'est pas complètement séquencé mais seulement un gêne l'est, l'ARN 16S.\\
Les séquences (reads) obtenues suite au séquencage sont assignées à une espèce bactérienne. Une fois cette étape effectuée, la table de comptage des OTUS est contruite (un OTU est un ensemble de bactéries qui se ressemblent).\\
La table de comptage des OTUS n'est autre qu'un tableau contenant le nombre de séquences par OTU observés dans chaque échantillon.


\subsection{Problématique des données de comptage}
L'une des problématiques pour l'étude statistique des données issues de la métagénomique est due à la différence du nombre d'OTU par échantillon. En effet, les séquenceurs hauts débits offrent la possibilité de séquencer un grand nombre d'échantillons en même temps, mais ne garantit pas le nombre de séquences (profondeur de séquençage) obtenues par échantillon. Le nombre d'OTU obtenus suite au séquencage dans un échantillon peut être par exemple de 1000 tandis que dans un autre il ne peut être que de 500.\\
La comparaison directe des échantillons est donc impossible.\\
Le passage aux données compositionnelles est une méthode de résolution possible de cette problématique.\\

\subsection{Objectif du stage}
Le but de ce stage est de transformer ces données de comptage en données compositionnelles et d'utiliser les méthodes d'analyse associées à ces données pour pouvoir leur appliquer un modèle probabiliste. \\
Ce modèle probabiliste a ensuite servi à développer un simulateur de nouvelles données.

\section{Géométrie de Aitchison}
Commençons par expliquer ce que sont les données compositionnelles.
\subsection{Simplexe}
Un vecteur \(x=\left[x_1,x_2,...x_D\right]\) est défini comme une D composition lorsque toutes ses composantes sont strictement positives et ne contiennent que de l'information relative, par exemple un pourcentage, une proportion.\\
\vspace*{0.3cm}. 
\\
Le simplexe est l'espace de probabilité des données compositionnelles  et est défini par: 
\[
    S^D=\lbrace x=\left[x_1, x_2,...,x_D\right]|x_i>0,i=1,2,...,D;\sum_{i=1}^Dx_i=\kappa\rbrace
\]
 
 L'opération permettant de modifier la somme des composantes (ex:passer d'une proportion à un pourcentage) d'une composition (le \(\kappa\)) se nomme la closure:
  
 \[
C(x)=\left[\frac{\kappa*x_1}{\sum_{i=1}^Dx_i},\frac{\kappa*x_2}{\sum_{i=1}^Dx_i},...,\frac{\kappa*x_D}{\sum_{i=1}^Dx_i}     \right]
\]
\vspace*{0.3cm}
Deux vecteurs \(x,y\in\mathbb{R}^D_{+}\) tels que \(x_i,\ y_i>0,\ \forall i=1,...,D\) sont dits compositionnellement équivalents s'il existe un \(\lambda\in \mathbb{R}^{+}\) tel que \(x=\lambda y\) ce qui est équivalent à \(C(x)=C(y)\).\\

La représentation graphique d'une donnée compositionnelle à trois composantes se fait grâce à un ternary diagram.\\
Un ternary diagram est un triangle équilateral ; pour une composition
\(X=\left[x_1, x_2, x_3\right]\), les composantes sont la distance entre sa  représentation graphique et un des côtés du triangle, comme montré dans le graphique suivant


\begin{figure}[H]
\begin{center}
<<fig.height=3>>=
par(mar=rep(0,4))
ternary_diagram2()
@
\end{center}
\end{figure}


\subsection{Opération dans le simplexe}
Dans le simplexe, nous ne pouvons pas utiliser la géométrie euclidienne. Remarquons cela en prenant un exemple, soit 4 compositions:\\
\(
x_1=\left[0.1,0.4,0.5\right],\ x_2=\left[0.2,0.3,0.5\right],\ x_3=\left[0.4,0.4,0.2\right],\ x_4=\left[0.5,0.3,0.2\right]
\)
\\
La distance euclidienne entre la composition \(x_1\) et \(x_2\) est la même que celle entre \(x_3\) et \(x_4\), pourtant la proportion de la première composante a doublé dans le premier cas mais est loin d'avoir doublé dans le second.\\
C'est une des raisons pour laquelle nous avons besoin de définir une nouvelle géométrie.
\\
\\
Commençons par définir les opérations de bases:\\

Perturbation d'une composition \(x\in S^D\) par une composition \(y\in S^D\),
\[
x\oplus y=C\left[x_1y_1,x_2y_2,...,x_Dy_D\right]
\]

La puissance d'une composition \(x\in S^D\) par une constante \(\alpha\in \mathbb{R}\), 
\[
x\in S^D,\ \alpha\in \mathbb{R},\ \alpha\odot =C\left[x_1^{\alpha},x_2^{\alpha},...,x_D^{\alpha}\right]
\]
Les deux opérations ci dessus permettent au simplexe de devenir un espace vectoriel (la perturbation en tant que loi de composition interne, la puissance en tant que loi de composition externe).\\
A cela, on ajoute un produit scalaire ainsi qu'une distance et une norme associée.\\

Inner Product de deux composition \(x,y\in S^D\),
\[
x,y\in S^D,\ \left\langle x,y\right\rangle_a=\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^Dln
\frac{x_i}{x_j}ln\frac{y_i}{y_j}
\]

Distance entre \(x\) et \(y\in S^D\),
\[
x,y\in S^D,\ d_a\left(x,y\right)=\sqrt{\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^D\left(ln\frac{x_i}{x_j}-ln\frac{y_i}{y_j}\right)^2}
\]

Norme d'une \(x\in S^D\),
\[
\left\|x\right\|_a=\sqrt{\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^D\left(ln\frac{x_i}{x_j}\right)^2}
\]

Il devient ainsi possible de définir des objets géométriques telles que les lignes compositionnelles (droite dans l'espace euclidien): \(y=x_0 \oplus \left(\alpha\odot x\right)\), avec \(x,x_0 \in S^D\) et \(\alpha in \mathbb{R}\). Ce qui donne graphiquement pour D=3,


\begin{figure}[H]
\begin{center}
<<fig.height=2>>=
par(mar=c(rep(0,4)))
ternary_diagram3()
@
\caption{Lignes compositionnels}
\end{center}
\end{figure}


Les mesures statistiques telles que la moyenne et la variance doivent être elles aussi redéfinies.\\

Pour un échantillon de taille \(n\) de données compositionnelles à \(D\) composantes, la moyenne est définie comme:
\[
g=C\left[g_1,g_2,...,g_D\right]
\]
avec \(g_i=\left(\prod_{j=1}^n x_{ij}\right)^{1/n},\ i=1,2,..,D\)
\\
La matrice de covariance quant à elle est définie par
\[
T=\begin{pmatrix}
t_{11} & t_{12} & ... & t_{1D} \\
...  & ... & ... & ...\\
t_{D1} & t_{D2} & ... & t_{DD}
\end{pmatrix}
,\quad t_{ij}=var\left(ln\frac{x_i}{x_j}\right),
\]
ce qui permet de définir la variance totale \[ totvar\left[X\right]=\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^Dt_{ij} \] \\
Pour centrer des données, il est alors nécessaire d'appliquer la perturbation avec \(g^{-1}\). \\
De même, appliquer la puissance avec \(totvar\left[X\right]^{-1/2}\) permet de les réduire. 


<<>>=
n <- 20
data <- matrix(c(runif(n,1,7),runif(n,0.1,1),runif(n,1,7)),nrow=n)
@

\begin{minipage}{\linewidth}
      \centering
      \begin{minipage}{0.45\linewidth}
          \begin{figure}[H]
          \captionsetup{justification=centering}
          <<fig.height=4.5>>=
          		par(mar=c(rep(0,4)))
              ternary_diagram(data %>% closure(), colour="red", type_point = 16)
              @
              \caption{Avant transformation ilr}
          \end{figure}
      \end{minipage}
      \hspace{0.05\linewidth}
      \begin{minipage}{0.45\linewidth}
          \begin{figure}[H]
          \captionsetup{justification=centering}
          <<fig.height=4.5>>=
          par(mar=c(rep(0,4)))
              ternary_diagram(data %>% closure() %>% center_scale(), colour="red", type_point = 16)
              @
              \caption{Après transformation ilr}
          \end{figure}
      \end{minipage}
  \end{minipage}\\

\subsection{Transformation}
Comme nous avons pu le voir, l'espace du simplexe nécessite une redéfinition des opérations de base. Dans cette partie, nous allons voir des transformations permettant de repasser dans un espace euclidien.\\
Tout d'abord, remarquons que la valeur de \(\kappa\)(somme des composantes) d'une composition n'a pas fondamentalement une grande importance. En effet, l'étude de données qu'elles soient exprimées en pourcentages ou en proportions ne change rien. \\
Les rapports relatifs sont bien plus intéressants. Il n'est donc pas étonnant que les transformations reposent sur des log ratios de composantes.\\

Commençons par l'additive log-ratio transformation(alr).
\[
alr:S^D\rightarrow R^{D-1},\ alr\left(x\right)=\left[ln\left(\frac{x_1}{x_D}\right), ln\left(\frac{x_2}{x_D}\right),...,ln\left(\frac{x_{D-1}}{x_D}\right)\right]
\]
La dernière composante d'une composition pouvant s'exprimer comme 
\[
x_D=\kappa -\sum_{i=1}^{D-1}x_i
\]
, le système sera alors lié. L'alr résout ce problème en prenant une composante comme dénominateur (la coordonnée est ainsi nulle).
Cependant, cette transformation est grandement dépendante de la composante choisie comme dénominateur. De plus, cette transformation ne préserve pas les distances.\\
\\
La transformation centred log-ratio n'a pas cette problématique.
Elle s'exprime comme suit:
\[
clr\left(x\right)=\left[ln\frac{x_1}{g(x)},...,ln\frac{x_D}{g(x)}\right]=\xi
\]	
avec \(g\) le centre de la composition définie par:
\[
 g(x)=\left(\prod^D_{i=1}x_i\right)^{1/D}
\]
Cependant, comme vu précédemment, la dernière composante peut s'exprimer en fonction des autres. Cette transformation conservant le nombre de coordonnées, les coordonnées ne seront pas indépendantes (la somme des coordonnées est égale à 0). En conséquence, la matrice de covariance est singulière. 


Voyons enfin l'isometric log-ratio transformation (ilr),
\[
ilr\left(x\right)=clr\left(x\right)\Psi^{'}
\]
avec \(\Psi\) l'image \(clr\) de la base.\\
Cette transformation permet de résoudre les problématiques des autres transformations, elle préserve les distances et ne génère pas une matrice de covariance liée. Pour cela, elle utilise un type de coordonnée particulier nommé balance.\\

Prenons comme système générateur, les vecteurs

\[
W_i=C\left(1,1,...,e,...\right),\quad i=1,...D-1
\]

En prenant l'image \(clr\) de ces vecteurs, puis en utilisant Gram-Schmidt  pour l'orthogonaliser, on obtient la matrice \(\Psi\).\\
Exemple de matrice \(\Psi\) en dimension 5:

\[
\Psi =\begin{pmatrix}
\frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{-2}{\sqrt{5}}\\
\frac{1}{\sqrt{12}} & \frac{1}{\sqrt{12}} & \frac{1}{\sqrt{12}} & 
-\frac{\sqrt{3}}{\sqrt{4}} & 0 \\
\frac{1}{\sqrt{6}} & \frac{1}{\sqrt{6}} & -\frac{\sqrt{2}}{\sqrt{3}} & 0 & 0 \\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} & 0 & 0 & 0
\end{pmatrix}
\]

Si nous cherchons une interprétation des coordonnées obtenues par la transformation \(ilr\), nous pouvons regarder la matrice des séquences binaires associée à la base.\\
Si nous reprenons l'exemple, nous avons comme matrice binaire

\[
\begin{blockarray}{ccccc}
x1 & x2 & x3 & x4  & x5 \\
\begin{block}{(ccccc)}
1 & 1 & 1 & 1 & -1 \\
1 & 1 & 1 & -1 & 0 \\
1 & 1 & -1 & 0 & 0 \\
1 & -1 & 0 & 0 & 0 \\
\end{block}
\end{blockarray}
\]
La première coordonnée ilr représente ainsi le ratio des quatre premières composantes de la composition sur la cinquième.\\
La deuxième : les trois premières composantes sur la quatrième et ainsi de suite.\\
D'où le nom de balance. Une coordonnée ilr négative (balance négative) signifie que le groupe des composantes du dénominateur à plus de poids que le groupe des composantes du numérateur dans la donnée compositionnelle.
\\
\\
Ces transformations permettent de passer dans un espace euclidien et de retrouver les formes géométriques habituelles.\\

<<>>=
c <- matrix(c(c(1,2,7)/10, ilr_inverse(c(-1.2,0)), c(1,8,1, 10/3, 10/3, 10/3)/10), byrow=TRUE, nrow=4)
l <- ligne(1.2,c(0.2,1,3))
y <- seq(0, 2*pi, length=1000)
circl <- cbind(1*cos(y)-1.2, 1*sin(y))
@

\begin{minipage}{\linewidth}
      \centering
      \begin{minipage}{0.35\linewidth}
          \begin{figure}[H]
          \captionsetup{justification=centering}
              <<fig.height=5.5>>=
              par(mar=rep(0,4))
              ternary_diagram1(c, colour=c(11, 2, 4, 1), style=c(16,16, 16, 3),add_line = l, colour_line = 15,             add_circle = (circl%>% ilr_inverse()), colour_circle = 14,               asp = 1, axes = FALSE)
              @
              \caption{Avant transformation ilr}
          \end{figure}
      \end{minipage}
      \hspace{0.05\linewidth}
      \begin{minipage}{0.55\linewidth}
          \begin{figure}[H]
          \captionsetup{justification=centering}
              <<fig.height=3>>=
              par(mar=rep(0.1,4))
              plot(c %>% ilr(), col=c(11, 2, 4, 1), pch=c(16, 16, 16, 3), cex=1.5, xlim = c(-2.2,2), ylim=c(-1,1.5), asp=1, xlab = "", ylab="")
lines(l %>% ilr(), col=15, lwd=3)
lines(circl, col=14, lwd=2)
              @
              \caption{Apres transformation ilr}
          \end{figure}
      \end{minipage}
  \end{minipage}\\

Ces trois transformations ont bien entendu leur transformation inverse qui sont définies par:\\
\begin{align*}
&alr^{-1}\left(\xi\right)=C\left( \frac{\exp\left(\xi_1\right)}{\sum_{i=1}^{d-1} \exp\left(\xi_i\right) + 1},\ ...,\  \frac{1}{\sum_{i=1}^{d-1} \exp\left(\xi_i\right) + 1} \right)\\
&clr^{-1}\left(\xi\right)=C\left( \exp\left(\xi_1\right),\ ...,\  \exp\left(\xi_D\right)\ \right)\\
&ilr^{-1}\left(\xi\right)=C\left(\exp{\xi\Psi}\right)
\end{align*}

\section{Analyse Multivariée}
\subsection{Réduction de dimension}
L'ACP est une méthode de réduction de dimension couramment utilisée pour synthétiser l'information.\\
Dans "Lecture Notes on Compositionnal Data Analysis", ils redéfinissent l'acp dans les données compositionnelles.\\
En effet, au lieu de réaliser la transformation puis de centrer et réduire les données, ils les centrent et les réduisent (avec les formules définies dans la géométrie de Aitchison) avant de réaliser la transformation ; ce qui est équivalent.

\subsection{Maximum a posteriori}
Nos jeux de données étant des jeux de données de comptage, de nombreux zéros sont présents comme expliqué précédemment ; les transformations logarithmiques (ilr, clr) ne peuvent ainsi pas être directement appliquées.\\
Pour ce faire, nous n'allons pas utiliser l'estimateur du maximum de vraisemblance pour estimer la proportion de chaque OTU dans un échantillon.\\
En mettant une distribution a priori sur les observations \(X\) et en utilisant le théorème de Bayes, nous avons
 
 
\begin{align*}
P\left(\theta|X\right)&=\frac{P\left(X|\theta\right)P\left(\theta\right)}{P\left(X\right)} \\
&\propto P\left(X|\theta\right)P\left(\theta\right)
\end{align*}

Dans notre cas, nous allons prendre un a priori de Dirichlet sur les données,
ce qui mène à l'estimateur suivant:
\[
MAP(p)=\underset{p}{argmax}\left(p|x\right)=\frac{x_i+1}{\sum_{i=1}^D\left(x_i+1\right)}
\]
 Cela revient simplement à ajouter un comptage pour chaque OTU.

\section{Transformation ilr et classification}

\subsection{Jeux de données}
Cinq jeux de données sont mis à ma disposition qui sont Chaillou\cite{Chaillou}, Mach\cite{Mach}, Liver, Ravel\cite{Ravel} ainsi que Vacher\cite{Vacher}. Tous ces jeux de données se présentent sous la forme de table de comptage d'OTU.\\

\subsubsection*{Chaillou}
Il s'agit d'un jeu de données d'origine alimentaire, il provient d'une étude sur les communautés bactériennes dans la nourriture périmée.\\
Pour cela, quatre types de viande : boeuf hache, veau hache, lardons, saucisses de volaille et quatre types de fruit de mer: saumon fumé, crevettes, filet de saumon et filet de cabillaud ont été échantillonnés.\\
Pour chaque type de nourriture huit échantillons (de lots différents) sont présents dans ce jeu de données. 508 OTU ont été conservés lors de l'étude.\\
Durant mon stage, le type de nourriture a été la variable qualitative étudiée.


\subsubsection*{Mach}
L'évolution du microbiote intestinal de porcelet au cours des premiers mois (avant, après le sevrage) ainsi que son impact sur le métabolisme était le sujet de l'étude contenant ce jeu de données.\\
Les excréments de 31 porcelets ont été séquencés à cinq périodes de leur vie (14, 36, 48, 60 et 70 jours) ; à chaque séquençage le sevrage ou non de l'animal a été relevé. Le jeu de données contient ainsi 155 échantillons, le nombre d'OTU quant à lui est 4031.\\


\subsubsection*{Liver}

\subsubsection*{Ravel}
Le microbiote vaginal permet-il de prévenir les maladies urogenitales ? C'était la question que se posaient les chercheurs dans l'étude contenant les données.\\
Pour répondre à cette question, ils ont séquencé le microbiote vaginal de 394 femmes provenant de quatre catégories ethniques différentes (Asiatiques, blanches, noires et hispanique), 247 OTU ont été gardés pour réaliser cette étude. Sur chaque échantillon (un pour chaque femme), le nugent score a été réalisé, il s'agit d'une méthode de diagnostic de la vaginose bactérienne.\\
Selon le résultat de ce test, les échantillons ont été classés en 3 catégories (low, intermediate, high).
De plus, durant cette étude, les échantillons ont été classés dans cinq clusters, ces clusters ont été les groupes étudiés durant mon stage.


\subsubsection*{Vacher}
L'objectif de l'étude d'où provienne les données était de trouver les interactions les plus probables entre un champignon responsable de l'oidium du chêne (Erysiphe alphitoides) et les autres espèces de microorganisme présentes sur les feuilles de chêne (Erysiphe alphitoides).\\
Dans cet objectif, les feuilles de trois chênes ont été prélevées et séquencées (40 sur chaque arbre). Pour chaque arbre, les  feuilles proviennent de quatre branches différentes (10 pour chaque branche) ; pour chaque feuille plusieurs caractéristiques ont été relevées: sa distance à la base de l'arbre, sa distance au tronc de l'arbre et sa distance par rapport au sol ainsi que son orientation (South west, North East) et enfin son niveau d'infection.\\
Cependant dans le jeu de données, seuls 116 échantillons de feuilles sont présents (il en manque quatre), sur chaque échantillon le nombre de 114 OTU est donné.

L'objectif durant mon stage était de retrouver pour chaque échantillon l'arbre auquel il appartenait.



Pour Liver, Mach, le nombre d'OTU étant très grand, j'ai choisi d'en enlever une partie (ceux présents dans moins de \(5\%\) des échantillons).\\
Les nouvelles dimensions des jeux de données sont ainsi: 155 échantillons sur 1084 OTU pour mach, 237 échantillons et 533 OTU pour liver. \\
\\
Dans les parties suivantes, seuls les résultats sur les jeux de données Vacher et Liver seront présentés. Les résultats sur les autres jeux de données sont donnés en annexe. 


\subsection{Graphique ACP}
Pour avoir un premier aperçu des jeux de données, faisons une acp sur les données compositionnelles et regardons la projection des échantillons sur les deux premiers axes.

<<graph biplot vacher, fig.height = 4.5>>=
graph_biplot_normale(vacher, metadata_vacher$tree, 1, "vacher", "tree", ellipse=FALSE) [[1]]
@

\begin{figure}[H]
<<graph biplot liver, fig.height=4.5>>=
graph_biplot_normale(liver_500, metadata_liver$status, 1, "liver", "status", ellipse=FALSE)[[1]]
@
\end{figure}


Pour vacher la séparation des échantillons issus des différents arbres est très franche.\\
Dans le cas des données de liver, la séparation est cette fois ci moins franche, mais reste quant même convenable.

\subsection{Classification non supervise}
Dans chacun des jeux de données utilisés, les groupes semblent relativement bien séparés comme vu précédement (tout du moins sur les premiers axes d'une acp après transformation ilr).\\
Une des questions que nous pouvons alors nous poser est : le passage aux données compositionnelles ( et transformation ilr) permet il de mieux distinguer les groupes?\\
Une classification non supervisée répond bien à cette question, ainsi j'ai utilisé plusieurs algorithmes (k-means, Mélange gaussien, classification hiérarchique) et regardé s'ils ont tendance à regrouper les échantillons issus des mêmes groupes.\\
Pour montrer l'intérêt de passer par les données compositionnelles pour la classification, j'ai comparé les résultats d'une classification effectuée sur les données initiales (de comptage) et les données compositionnelles (après transformation \(ilr\)).\\


\begin{center}
Vacher

\begin{tabular}{cc}
\multicolumn{2}{c}{kmeans} \\
comptage & ilr \\
<<>>=
kable(k_vacher$comptage_table)
@
&
<<>>=
kable(k_vacher$ilr_table)
@
\end{tabular}

\begin{tabular}{cc}
\multicolumn{2}{c}{hiérarchique} \\
comptage & ilr \\
<<>>=
kable(hclust_vacher$comptage_table)
@
&
<<>>=
kable(hclust_vacher$ilr_table)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{Mélange gaussien} \\
comptage & ilr \\
<<>>=
kable(Mclust_vacher$comptage_table)
@
&
<<>>=
kable(Mclust_vacher$ilr_table)
@
\end{tabular}
\end{center}




\begin{center}
Liver

\begin{tabular}{cc}
\multicolumn{2}{c}{kmeans} \\
comptage & ilr \\
<<>>=
kable(k_liver$comptage_table)
@
&
<<>>=
kable(k_liver$ilr_table)
@
\end{tabular}

\begin{tabular}{cc}
\multicolumn{2}{c}{hiérarchique} \\
comptage & ilr \\
<<>>=
kable(hclust_liver$comptage_table)
@
&
<<>>=
kable(hclust_liver$ilr_table)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{Mélange gaussien} \\
comptage & ilr \\
<<>>=
kable(Mclust_liver$comptage_table)
@
&
<<>>=
kable(Mclust_liver$ilr_table)
@
\end{tabular}
\end{center}

La transformation \(ilr\) semble améliorer sensiblement la classification non supervisée.\\
Les classificateurs ont tendance sur les données de comptage à toutes les mettre dans le même groupe.

 
\subsection{Classification supervise}
Regardons maintenant les performances de classification supervisée par le biais de trois algorithmes qui sont, le randomForest (avec la matrice de confusion), le support machine vector et le k nearest neighbors (en faisant pour les deux derniers une 10-fold cross validation). \\


\begin{center}
Vacher


\begin{tabular}{cc}
\multicolumn{2}{c}{random Forest} \\
comptage & ilr \\
<<>>=
kable(v_vacher$confusion_random_Forest)
@
&
<<>>=
kable(v_vacher_ilr$confusion_random_Forest)
@
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{cc}
\multicolumn{2}{c}{svm} \\
comptage & ilr \\
<<>>=
kable(v_vacher$Svm)
@
&
<<>>=
kable(v_vacher_ilr$Svm)
@
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{cc}
\multicolumn{2}{c}{kNN} \\
comptage & ilr \\
<<>>=
kable(v_vacher$kNN)
@
&
<<>>=
kable(v_vacher_ilr$kNN)
@
\end{tabular}
\end{center}


\begin{center}
Liver

\begin{tabular}{cc}
\multicolumn{2}{c}{random Forest} \\
comptage & ilr \\
<<>>=
kable(v_liver_500$confusion_random_Forest)
@
&
<<>>=
kable(v_liver_500_ilr$confusion_random_Forest)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{svm} \\
comptage & ilr \\
<<>>=
kable(v_liver_500$Svm)
@
&
<<>>=
kable(v_liver_500_ilr$Svm)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{kNN} \\
comptage & ilr \\
<<>>=
kable(v_liver_500$kNN)
@
&
<<>>=
kable(v_liver_500_ilr$kNN)
@
\end{tabular}
\end{center}
Dans le cas de certains classificateurs tel que le svm, la classification supervisée sur certains jeux de données de comptage ne donne pas du tout des résultats concluants (voir chaillou en Annexe \ref{annexe classification supervise}).\\
Que ce soit pour la classification supervisée ou non supervisée, les résultats sont meilleurs sur les données compositionnelles (après transformation ilr) que sur les données initiales. Il semble donc avoir un intérêt à passer par les données compositionnelles pour la classification.

\section{Simulateur}

\subsection{Schéma simulateur}
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]
     \node[cloud] (X) {X};
     \node[block, below of=X, text width=3cm] (apprent_ZIB) {apprentissage zero inflated};
     \node[cloud, below of=apprent_ZIB] (S)      {S};
     \node[cloud, below of=S] (Y)      {Y};
     \node[cloud, below of=Y] (Z) {Z};
     \node[block, below of=Z, text width=3cm](apprentissage) {apprentissage de la densité};
     \node[cloud, below of=apprentissage] (Z_tilde) {\(\tilde{Z}\)};
     \node[cloud, below of=Z_tilde] (Y_tilde) {\(\tilde{Y}\)};
     \node[cloud, below of=Y_tilde] (S_tilde) {\(\tilde{S}\)};
     \node[block, below of=S_tilde](ZIB) {Zero inflated};
     \node[cloud, below of=ZIB] (X_tilde) {\(\tilde{X}\)};
     
     \node[right of=Z_tilde, node distance=1.7cm] (N) {\(\sim \mathbb{N}\left(\mu_k, \Sigma_k \right)\)};
     \node[right of=Y_tilde, node distance=3cm] (N1) {\(\sim \mathbb{N}\left(W\mu_k, W\Sigma_k W^T+\sigma^2I \right)\)};
     
     
     \node[left of=apprentissage] (temp1_simulation) {};
     \node[left of=X_tilde] (temp2_simulation) {};
  %\draw[tuborg, decoration={brace}] let %\p1=(Y.north), \p2=(Y_tilde.south) in
%    ($(2, \y1)$) -- ($(2, \y2)$) node[tubnode] {Pro};
    
  \path[line] (X) -- (apprent_ZIB);
  \path[line] (apprent_ZIB) --  node[midway, right] {MAP} (S);
  \path[line] (S) --  node[midway, right] {\(ilr\)} (Y);
  \path[line] (Z) --  node[midway, right] {} (apprentissage);
  \path[line] (apprentissage) -- node[midway, right] {simulation gaussienne} (Z_tilde);
  \path[line] (Z_tilde) --  node[midway, right] {} (Y_tilde);
  \path[line] (Y_tilde) --  node[midway, right] {\(ilr^{-1}\)} (S_tilde);
  \path[line] (S_tilde) --  node[midway, right] {multinomiale} (ZIB);
  \path[line] (ZIB) --  node[midway, right] {} (X_tilde);
  \path[line] (Y) -- node[midway, right] {réduction de dimension} (Z);
  
\draw[decorate,decoration={brace,mirror}] (temp1_simulation.north west) -- node[midway, left] {Simulation} (temp2_simulation.south west);
  \end{tikzpicture}
\end{center}


\subsection{Réduction de dimension}
Les jeux de données utilisés dans le cadre du simulateur ont un très grand nombre de variables (grande dimension); une réduction de dimension s'impose.\\
Dans notre cas, nous cherchons à appliquer un modèle probabiliste à nos données. L'ACP dans sa définition la plus commune ne permet pas de prendre en compte un tel modèle. Pour ce faire, nous allons donc utiliser une variante de l'ACP qui n'est autre que l'ACP probabiliste qui est définie comme:
\[
X_i=Wy_i + \mu + \epsilon_i
\]
où \(y_i\sim \mathcal{N}\left(0,I_d\right) \), \(\epsilon_i \sim \mathcal{N}\left(0,\ \sigma^2I_p\right)\) est un bruit gaussien et \(W\) une \(p \times d\) matrice.
\\
Si nous notons \(A\) la matrice des vecteurs propres de la matrice \(X^TX\), \(\Lambda\) la matrice diagonale contenant les valeurs propres associées et \(R\) une matrice orthogonale arbitraire, nous avons que le maximum de vraisemblance de \(W\) s'exprime comme:

\[
W_{ML}=A\left(\Lambda-\sigma^2I_d\right)^{1/2}R
\]
De même, l'estimateur du maximun de vraisemblance de \(\sigma^2\)
\[
\sigma^2_{ML}=\frac{1}{d-q}\sum_{j=q+1}^d \lambda_j
\]
Cet estimateur peut être perçu comme la variance perdue lors de la projection.\\

La dimension de l'espace latent a été choisie à l'aide de l'heuristique de pente (grâce au package capushe).\\
\subsection{Mélange Gaussien}
L'apprentissage de la densité est confié à un mélange gaussien.\\
Un mélange gaussien est défini comme:
\[
g\left(x; \pi,\theta\right)=\sum_{k=1}^K \pi_kf(x,\theta_k)
\]
où \(\theta=\left(\theta_1,...,\theta_k\right)\) sont les paramètres des lois gaussiennes et \( \pi=\left(\pi_1,...,\pi_K\right)\) sont les proportions du mélange Gaussien dont la somme est égale à 1.\\
\\
Voici des exemples de mélange gaussien:

  \begin{minipage}{\linewidth}
      \centering
      \begin{minipage}{0.45\linewidth}
          \begin{figure}[H]
          \captionsetup{justification=centering}
              <<fig.height=6.5>>=
              par(mar=rep(4,4))
              x <- seq(-5, 6, length=10000)
				y1 <- dnorm(x, mean=-1.8)*0.3
				y2 <- dnorm(x, mean=1.6, sd=1.4)*0.5
				y3 <- dnorm(x, mean=-3, sd=0.4)*0.2

				plot(x, y1, type='l', col='red', ylim=c(0,0.3), 						xlim=c(-4,5), ylab="y")
				lines(x, y2, col='blue')
				lines(x,y3, col="green")
				lines(x, y1+y2+y3)
				legend("topright", lty=c(1,1,1), col=c("red", 							"blue", "green", "black"), legend=c("G1","G2", 							"G3","G1+G2+G3"))

              @
              \caption{Mélange de trois gaussiennes \newline univariée}
          \end{figure}
      \end{minipage}
      \hspace{0.05\linewidth}
      \begin{minipage}{0.45\linewidth}
          \begin{figure}[H]
          \captionsetup{justification=centering}
             \includegraphics[width=\linewidth]{melangegaussien.png}
              \caption{Mélange de trois gaussiennes \newline multivariée}
          \end{figure}
      \end{minipage}
  \end{minipage}\\
Pour le simulateur, le nombre de gaussiennes nécessaires à l'estimation de la densité est choisi en minimisant le critère BIC.\\

\subsection{Zero inflated}
Les jeux de données contiennent un nombre excessif de zéros, la modélisation faite par un mélange gaussien ne parvient pas à bien prendre en compte tous ces zéros. Pour résoudre ce problème, nous allons modéliser cette particularité indépendamment. \\
Commençons par estimer la proportion de zéros pour chaque OTU de chaque gaussienne que nous notons \(\pi_{jk}\) (j désignant l'OTU et k la gaussienne d'appartenance de l'échantillon). Cette estimation est faite via le MAP comme pour la proportion d'un OTU dans un échantillon mais cette fois ci en prenant un pseudo comptage plus faible.\\
Par la suite, nous modélisons les zéros par une loi de Bernoulli,

\[P\left(Z=x\right)=\left\{
    \begin{array}{ll}
         \pi_{ik} & \text{ si } x=0 \\
         1 - \pi_{ik} & \text{ si } x=1 \\
    \end{array}
\right.
\]
Ce qui permet d'exprimer \(\tilde{X_{i}}\) comme 

\[
\tilde{X}_{i}=\left(Z_{1k},Z_{jk},...,Z_{Dk}\right)*T_i
\]
où \(Z_{ik}\sim Bern\left(\pi_{jk}\right)\) et \(T_i\sim \mathbb{M}\left(N; \tilde{S}_i\right)\) avec \\
 \(\tilde{S}_i\) qui est le résultat de \(ilr^{-1}\left(\mathbb{N}\left(W\mu_k, W\Sigma_k W^T+\sigma^2I \right)\right)\)
 

\section{Performance simulateur}
\subsection{Méthode}
La méthode la plus facile à mettre en œuvre est de regarder visuellement si les données simulées ressemblent aux données réelles, pour ce faire j'ai regardé les projections des données (réelles, simulées) sur les premiers axes d'une acp.

\begin{figure}[H]
<<graphique bootstrap vacher, fig.height = 4.5>>=
data <- rbind(vacher, vacher_boot$data)
metadata <- c(as.character(rep('real', nrow(vacher))), rep('simu', nrow(vacher_boot$data))) %>% as.factor()
graph_biplot_normale(data, metadata, 1, 'Vacher', 'data', ellipse=FALSE)[[1]]
@
\end{figure}

\begin{figure}[H]
<<graphique bootstrap liver, fig.height = 4.5>>=
data <- rbind(liver_500, liver_boot$data)
metadata <- c(as.character(rep('real', nrow(liver))), rep('simu', nrow(liver_boot$data))) %>% as.factor()
graph_biplot_normale(data, metadata, 1, 'Liver', 'data', ellipse=FALSE)[[1]]
@
\end{figure}

Globalement, les données simulées se mélangent bien avec les données réelles.
La première impression sur les performances du simulateur sont positives.\\
\\
Cependant cette méthode n'est pas forcément la plus efficace. En effet, certains OTU peuvent être très mal simulés sans que pour autant nous puissions faire la différence visuellement. Par exemple, un OTU absent de tous les échantillons réels mais présent en de très faibles proportions dans les échantillons simulés n'aura aucune incidence sur une acp, il n'est pas pour autant bien simulé.\\
\\
Pour avoir des résultats plus fiables, l'étude de l'efficacité du simulateur est confiée à des classificateurs.\\
L'objectif est de regarder si un classificateur parvient à faire la différence entre les données réelles et les données simulées.\\
Dans cet objectif, deux nouveaux jeux de données sont simulés, l'un d'entre eux servira de jeu d'entrainement (avec les données réelles) à un classificateur, l'autre jeu de données servira quant à lui de test. Une petite partie des données réelles n'est pas incluse dans le jeu d'entrainement, elle servira elle aussi de test.\\


\subsection{Résultats}
J'ai utilisé deux classificateurs, le random Forest ainsi que le k nearest neighbors (le classificateur svm a été ignoré pour sa faible efficacité pour les données de comptage).\\


\begin{center}
Vacher

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(t_vacher$all$random_forest)
@
&
<<>>=
kable(t_vacher$all$kNN)
@
\end{tabular}
\end{center}


\begin{center}
 Liver
 \begin{tabular}{cc}
 Random forest & kNN \\
 <<>>=
kable(t_liver_500$all$random_forest) 
 @
 &
 <<>>=
 kable(t_liver_500$all$kNN)
 @
 \end{tabular}
\end{center}
Les performances du simulateur sont grandement variables selon le jeu de données. En effet, sur certains jeux de données les classificateurs peinent à faire la différence entre les données simulées et réelles.\\
A l'inverse sur Liver, le random Forest fait aisément la différence, cependant le kNN lui le fait moins facilement. Ainsi certaines caractéristiques des données Liver doivent être mal simulées (ce qui permet au random Forest de faire la différence), mais les données simulées doivent quant même relativement bien se mélanger aux données réelles (comme le montre le kNN).

\subsection{Classification}
Un autre aspect de la performance du simulateur est sa capacité à simuler les caractéristiques de chaque groupe d'un jeu de données ; une donnée simulée ne doit pas avoir les caractéristiques de plusieurs groupes.\\
Pour étudier cet aspect, la simulation ne se fera plus sur l'ensemble du jeux de données mais séparément sur chaque groupe du jeu de données.\\
Les groupes des données simulées seront alors connus et nous pourrons vérifier si un classificateur retrouve bien ses groupes.\\
Voici un schéma illustratif de la méthode \\
Soit \(1,2,...,k\) les groupes du jeu de données (ex: pour Liver k=2, malade, non malade)
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]
     \node[cloud] (X1) {\(X_1\)};
     \node[cloud, right of=X1] (X2)      {\(X_2\)};
     \node[right of=X2, node distance=1cm] (temp) {};
     \node[cloud, right of=temp, node distance=1cm] (point1) {...};
     \node[cloud, right of=point1] (Xk) {\(X_k\)};
     \node[cloud, below of=X2] (X2_tilde)   {\(\tilde{X}_2\)};
     \node[cloud, below of=X1] (X1_tilde)   {\(\tilde{X}_1\)};
     \node[below of=temp] (temp2) {};
     \node[cloud, below of=point1] (point2)   {...};
     \node[cloud, below of=Xk] (Xk_tilde)   {\(\tilde{X}_k\)};
     
     \node[cloud, below of=temp2](XU_tilde) {\(\tilde{X}_U\)};
     
     \node[below of=XU_tilde] (temp3) {};
     \node[cloud, left of=temp3, node distance=1cm] (Y2_tilde) {\(\tilde{Y}_2\)};
     \node[cloud, left of=Y2_tilde] (Y1_tilde) {\(\tilde{Y}_1\)};
     \node[cloud, right of=temp3, node distance=1cm] (point3) {...};
	 \node[cloud, right of=point3] (Yk_tilde) {\(\tilde{Y}_k\)};     
     
     
    % \draw[tuborg, decoration={brace}] let \p1=(X2.south), %\p2=(X2_tilde.north) in
  %  ($(2, \y1)$) -- ($(2, \y2)$) node[tubnode] {Pro};
     
   	\path[line] (X1) --  node[midway, left] {Apprentissage, simulation} (X1_tilde);
   	\path[line] (X2) --  (X2_tilde);
   	\path[line] (point1) -- (point2);
  	\path[line] (Xk) --  (Xk_tilde);
 	\path[line] (X1_tilde) --  node[midway, left] {Union} (XU_tilde);
 	\path[line] (X2_tilde) --  (XU_tilde);
 	\path[line] (point2) --  (XU_tilde);
 	\path[line] (Xk_tilde) -- (XU_tilde);
 	\path[line] (XU_tilde) -- node[midway, left] {Classification} (Y1_tilde);
 	\path[line] (XU_tilde) --  (Y2_tilde);
 	\path[line] (XU_tilde) --  (point3);
 	\path[line] (XU_tilde) --  (Yk_tilde);
 	
  \end{tikzpicture}
\end{center}
Une bonne simulation devrait résulter en la présence des mêmes échantillons dans \(\tilde{X}_i\) et dans \(\tilde{Y}_i\) pour \(i\in \left\lbrace 1,2,...,k\right\rbrace\).

\begin{center}
Vacher

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(v_super$all$random_forest)
@
&
<<>>=
kable(v_super$all$kNN)
@
\end{tabular}
\end{center}


\begin{center}
Liver

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(l_super$all$random_forest)
@
&
<<>>=
kable(l_super$all$kNN)
@

\end{tabular}
\end{center}
La classification supervisée des données simulées sont globalement bonnes. Les résultats sont même quasiment comparables aux résultats de la validation croisée sur les données réelles (pour le random Forest). Ainsi même si une partie des échantillons simulés est malheureusement "mal simulée" (notamment dans le cas du jeu de données liver); les caractéristiques des groupes restent quant à eux relativement bien simulés. \\


Pour vérifier que les données considérées comme bien "simulées" sont facilement classables dans un groupe (et ainsi relativement proches des données réelles de leur groupe respectif), nous allons conserver uniquement ces données puis faire comme précédemment en faisant une classification supervisée et en vérifiant leur bonne classification.

Voici un schéma récapitulatif de la méthode.
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]

\node[cloud] (X1) {\(X_1\)};
     \node[cloud, right of=X1] (X2)      {\(X_2\)};
     \node[right of=X2, node distance=1cm] (temp) {};
     \node[cloud, right of=temp, node distance=1cm] (point1) {...};
     \node[cloud, right of=point1] (Xk) {\(X_k\)};
     \node[cloud, below of=X2] (X2_tilde)   {\(\tilde{X}_2\)};
     \node[cloud, below of=X1] (X1_tilde)   {\(\tilde{X}_1\)};
     \node[below of=temp] (temp2) {};
     \node[cloud, below of=point1] (point2)   {...};
     \node[cloud, below of=Xk] (Xk_tilde)   {\(\tilde{X}_k\)};
     
     \node[cloud, below of=temp2](XU_tilde) {\(\tilde{X}_U\)};
     \node[below of=XU_tilde, node distance=1cm] (temp_real) {};
     \node[cloud, below of=temp_real, node distance=1cm] (X_real_tilde) {\(\tilde{X}_{real}\)};
     \node[below of=X_real_tilde] (temp3) {};
     \node[cloud, left of=temp3, node distance=1cm] (Y2_tilde) {\(\tilde{Y}_2\)};
     \node[cloud, left of=Y2_tilde] (Y1_tilde) {\(\tilde{Y}_1\)};
     \node[cloud, right of=temp3, node distance=1cm] (point3) {...};
	 \node[cloud, right of=point3] (Yk_tilde) {\(\tilde{Y}_k\)};     
     
     
    % \draw[tuborg, decoration={brace}] let \p1=(X2.south), %\p2=(X2_tilde.north) in
  %  ($(2, \y1)$) -- ($(2, \y2)$) node[tubnode] {Pro};
     
   	\path[line] (X1) --  node[midway, left] {Apprentissage, simulation} (X1_tilde);
   	\path[line] (X2) --  (X2_tilde);
   	\path[line] (point1) -- (point2);
  	\path[line] (Xk) --  (Xk_tilde);
 	\path[line] (X1_tilde) --  node[midway, left] {Union} (XU_tilde);
 	\path[line] (X2_tilde) --  (XU_tilde);
 	\path[line] (point2) --  (XU_tilde);
 	\path[line] (Xk_tilde) -- (XU_tilde);
 	\path[line] (XU_tilde) -- node[midway, left] {Classification real/simu} (X_real_tilde);
 	\path[line] (X_real_tilde) -- node[midway, left] {Classification} (Y1_tilde);
 	\path[line] (X_real_tilde) --  (Y2_tilde);
 	\path[line] (X_real_tilde) --  (point3);
 	\path[line] (X_real_tilde) --  (Yk_tilde);
 	
     
     
     
     \node[block, right of=XU_tilde, node distance=6cm, text width=3cm] (Apprentissage classificateur) {Apprentissage classificateur real/simu};
     \node[cloud, above of=Apprentissage classificateur] (X_tilde) {\(\tilde{X}\)}; 
     \node[cloud, above of=X_tilde] (X) {X};
     
     
     
     

 	\path[line] (Apprentissage classificateur) |- (temp_real);
 	\path[line] (X) -- (X_tilde);
 	\path[line] (X_tilde) -- (Apprentissage classificateur);
 	
  \end{tikzpicture}
\end{center}


\begin{center}
Vacher

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_vacher$all$random_forest)
@
&
<<>>=
kable(c_vacher$all$kNN)
@
\end{tabular}
\end{center}


\begin{center}
Liver

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_liver$all$random_forest)
@
&
<<>>=
kable(c_liver$all$kNN)
@
\end{tabular}
\end{center}
Les résultats sont comparables à ceux obtenus avec la méthode précédente.
Les données considérées comme réelles (bien simulées) ne semblent pas plus faciles, ni plus difficiles à classer que dans le cas précédent. Les jeux de données où les résultats sur l'ensemble des données simulées étaient déjà très convaincants le sont toujours ; mais les jeux de données où la classification des données simulées était plus compliquée reste compliquée (le classificateur kNN pour liver).\\
Cependant sur ces jeux de données, la validation croisée (Classification supervisée) sur les données réelles ne permet pas non plus d'obtenir de bons résultats, la problématique ne semble donc pas venir des données simulées.\\


\section{Conclusion}
L'intérêt des données compositionnelles est de pouvoir s'affranchir de certaines contraintes techniques comme la profondeur de séquencage.\\
L'utilisation des transformations des données compositionnelles pour repasser dans un espace euclidien s'est révélée pertinente sur les jeux de données utilisé durant mon stage, les résultats des différentes classifications étant au moins aussi bons que sur les données initiales.\\
De même les résultats obtenus sur les données simulées sont encourageants.\\
Les caractéristiques des différents groupes des jeux de données semblent relativement bien simulées. Cependant, les résultats sont très variables selon les jeux de données. Des tests sur un plus grand nombre de jeux de données seraient nécessaires pour étudier la robustesse du simulateur.

\newpage
\section{Annexe}
\appendix


\subsection{Graphique ACP}
\begin{figure}[H]
<<graph biplot chaillou, fig.height = 4.5>>=
graph_biplot_normale(chaillou, metadata_chaillou$EnvType, 1, "Chaillou", "EnvType", ellipse=FALSE)[[1]]
@
\end{figure}

\begin{figure}[H]
<<graph biplot ravel, fig.height = 4.5>>=
graph_biplot_normale(ravel, metadata_ravel$CST, 1, "Ravel", "CST", ellipse=FALSE) [[1]]
@
\end{figure}

\begin{figure}[H]
<<graph biplot mach_500, fig.height=4.5>>=
graph_biplot_normale(mach_500, metadata_mach$Weaned, 1, "Mach", "Weaned", ellipse=FALSE) [[1]]
@
\end{figure}


\subsection{Classification non supervise}\label{annexe classification non supervise}
\begin{center}
Chaillou

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{kmeans} \\
comptage & ilr \\
<<>>=
kable(k_chaillou$comptage_table)
@
&
<<>>=
kable(k_chaillou$ilr_table)
@
\end{tabular}
}


\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{hiérarchique} \\
comptage & ilr \\
<<>>=
kable(hclust_chaillou$comptage_table)
@
&
<<>>=
kable(hclust_chaillou$ilr_table)
@
\end{tabular}
}

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{Mélange gaussien} \\
comptage & ilr \\
<<>>=
kable(Mclust_chaillou$comptage_table)
@
&
<<>>=
kable(Mclust_chaillou$ilr_table)
@
\end{tabular}
}

\end{center}


\begin{center}
Ravel

\begin{tabular}{cc}
\multicolumn{2}{c}{kmeans} \\
comptage & ilr \\
<<>>=
kable(k_ravel$comptage_table)
@
&
<<>>=
kable(k_ravel$ilr_table)
@
\end{tabular}

\begin{tabular}{cc}
\multicolumn{2}{c}{hiérarchique} \\
comptage & ilr \\
<<>>=
kable(hclust_ravel$comptage_table)
@
&
<<>>=
kable(hclust_ravel$ilr_table)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{Mélange gaussien} \\
comptage & ilr \\
<<>>=
kable(Mclust_ravel$comptage_table)
@
&
<<>>=
kable(Mclust_ravel$ilr_table)
@
\end{tabular}
\end{center}


\begin{center}
Mach

\begin{tabular}{cc}
\multicolumn{2}{c}{kmeans} \\
comptage & ilr \\
<<>>=
kable(k_mach_500$comptage_table)
@
&
<<>>=
kable(k_mach_500$ilr_table)
@
\end{tabular}

\begin{tabular}{cc}
\multicolumn{2}{c}{hiérarchique} \\
comptage & ilr \\
<<>>=
kable(hclust_mach_500$comptage_table)
@
&
<<>>=
kable(hclust_mach_500$ilr_table)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{Mélange gaussien} \\
comptage & ilr \\
<<>>=
kable(Mclust_mach_500$comptage_table)
@
&
<<>>=
kable(Mclust_mach_500$ilr_table)
@
\end{tabular}
\end{center}






\subsection{Classification supervise}\label{annexe classification supervise}
\begin{center}
Chaillou

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{random Forest} \\
comptage & ilr \\
<<>>=
kable(v_chaillou$confusion_random_Forest)
@
&
<<>>=
kable(v_chaillou_ilr$confusion_random_Forest)
@
\end{tabular}
}
\end{center}


\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{svm} \\
comptage & ilr \\
<<>>=
kable(v_chaillou$Svm)
@
&
<<>>=
kable(v_chaillou_ilr$Svm)
@
\end{tabular}
}
\end{center}


\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{kNN} \\
comptage & ilr \\
<<>>=
kable(v_chaillou$kNN)
@
&
<<>>=
kable(v_chaillou_ilr$kNN)
@
\end{tabular}
}
\end{center}


\begin{center}
Ravel


\begin{tabular}{cc}
\multicolumn{2}{c}{random Forest} \\
comptage & ilr \\
<<>>=
kable(v_ravel$confusion_random_Forest)
@
&
<<>>=
kable(v_ravel_ilr$confusion_random_Forest)
@
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{cc}
\multicolumn{2}{c}{svm} \\
comptage & ilr \\
<<>>=
kable(v_ravel$Svm)
@
&
<<>>=
kable(v_ravel_ilr$Svm)
@
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{cc}
\multicolumn{2}{c}{kNN} \\
comptage & ilr \\
<<>>=
kable(v_ravel$kNN)
@
&
<<>>=
kable(v_ravel_ilr$kNN)
@
\end{tabular}
\end{center}



\begin{center}
Mach

\begin{tabular}{cc}
\multicolumn{2}{c}{random Forest} \\
comptage & ilr \\
<<>>=
kable(v_mach_500$confusion_random_Forest)
@
&
<<>>=
kable(v_mach_500_ilr$confusion_random_Forest)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{svm} \\
comptage & ilr \\
<<>>=
kable(v_mach_500$Svm)
@
&
<<>>=
kable(v_mach_500_ilr$Svm)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{kNN} \\
comptage & ilr \\
<<>>=
kable(v_mach_500$kNN)
@
&
<<>>=
kable(v_mach_500_ilr$kNN)
@
\end{tabular}
\end{center}


\subsection{Performance simulateur}
\subsubsection*{Graphique}
\begin{figure}[H]
<<graphique bootstrap chaillou, fig.height = 4.5>>=
data <- rbind(chaillou, chaillou_boot$data)
metadata <- c(as.character(rep('real', nrow(chaillou))), rep('simu', nrow(chaillou_boot$data))) %>% as.factor()
graph_biplot_normale(data, metadata, 1, 'Chaillou', 'data', ellipse=FALSE)[[1]]
@
\end{figure}

\begin{figure}[H]
<<graphique bootstrap ravel, fig.height = 4.5>>=
data <- rbind(ravel, ravel_boot$data)
metadata <- c(as.character(rep('real', nrow(ravel))), rep('simu', nrow(ravel_boot$data))) %>% as.factor()
graph_biplot_normale(data, metadata, 1, 'Ravel', 'data', ellipse=FALSE)[[1]]
@
\end{figure}


\begin{figure}[H]
<<graphique bootstrap mach, fig.height = 4.5>>=
data <- rbind(mach_500, mach_boot$data)
metadata <- c(as.character(rep('real', nrow(mach_500))), rep('simu', nrow(mach_boot$data))) %>% as.factor()
graph_biplot_normale(data, metadata, 1, 'Mach', 'data', ellipse=FALSE)[[1]]
@
\end{figure}


\subsubsection*{Sur le jeu de données entier}\label{annexe donnees entier}

\begin{center}
Chaillou

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(t_chaillou$all$random_forest)
@
&
<<>>=
kable(t_chaillou$all$kNN)
@
\end{tabular}
\end{center}

\begin{center}
Ravel

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(t_ravel$all$random_forest)
@
&
<<>>=
kable(t_ravel$all$kNN)
@
\end{tabular}
\end{center}

\begin{center}
Mach

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(t_mach_500$all$random_forest)
@
&
<<>>=
kable(t_mach_500$all$kNN)
@
\end{tabular}
\end{center}



\subsubsection*{Sur les groupes}\label{annexe sur les groupes}
\begin{center}
Chaillou

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_super$all$random_forest)
@
&
<<>>=
kable(c_super$all$kNN)
@
\end{tabular}
}
\end{center}


\begin{center}
Ravel

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(r_super$all$random_forest)
@
&
<<>>=
kable(r_super$all$kNN)
@
\end{tabular}
\end{center}


\begin{center}
Mach

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(m_super$all$random_forest)
@
&
<<>>=
kable(m_super$all$kNN)
@
\end{tabular}
\end{center}


\subsubsection*{Sur les données simulées considéré comme réels}\label{reels/simu}
\begin{center}
Chaillou

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_chaillou$all$random_forest)
@
&
<<>>=
kable(c_chaillou$all$kNN)
@
\end{tabular}
}
\end{center}


\begin{center}
Ravel

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_ravel$all$random_forest)
@
&
<<>>=
kable(c_ravel$all$kNN)
@
\end{tabular}
\end{center}


\begin{center}
Mach

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_mach$all$random_forest)
@
&
<<>>=
kable(c_mach$all$kNN)
@
\end{tabular}
\end{center}

\nocite{*}
\bibliographystyle{plain}
\bibliography{bibli.bib}

\end{document}
