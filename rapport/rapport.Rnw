\documentclass[11pt,a4paper]{article}
\usepackage{tikz}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage[caption = false]{subfig}
\usepackage{bbm}
\usepackage[singlespacing]{setspace}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{makecell} 
\usepackage{blkarray}
\usepackage{calc}
\usepackage{multirow}
\usepackage{array}
\usepackage{bigdelim}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes}

\tikzstyle{decision} = [diamond, draw, text width=4.5em, 
                        text badly centered, node distance=2cm, 
                        inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, 
                     text centered, rounded corners, 
                     minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, circle, minimum height=3em]

\newcommand{\ER}{\ensuremath{\mathbb{R}}}
\newcommand{\EE}{\ensuremath{\mathbb{E}}}

\tikzset{
>=stealth',
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

<<setup, include=FALSE, echo=FALSE>>=
library(knitr)
library(weaver)
library(kableExtra)
library(xtable)
library(doParallel)
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, fig.path = "figure/", cache.path = "cache/")
knitr::opts_knit$set(root.dir = "..")
@

\title{Exploration des méthodes d'analyse de données compositionnelles pour l'étude du lien entre microbiote intestinal et santé.}
\author{Clément Hardy}
\date{}

\begin{document}

<<chargement_source, echo=FALSE,  include=FALSE>>=
source("Rscript/read_metagenomic_data.R")
source("Rscript/coda.R")
source("Rscript/comparaison_clustering.R")
source("Rscript/graph.R")
source("Rscript/bootstrap.R")
source("Rscript/test_bootstrap.R")
source("Rscript/tree_phyloseq.R")
source("Rscript/apprentissage_supervise.R")
@

\maketitle

\tableofcontents


\section{Contexte biologique et enjeux}
\subsection{Séqueçage}
\subsection{Données de comptage}
\subsection{Problème compositionnel}


\section{Géométrie de Aitchison}

\subsection{Simplexe}
Un vecteur \(x=\left[x_1,x_2,...x_D\right]\) est défini comme une D composition lorsque toutes ses composantes sont strictement positives et ne contiennent que de l'information relative.
\vspace*{0.3cm}. 
\\
Le simplexe est l'espace de probabilité des données compositionnelles  et est défini par: 
\[
    S^D=\lbrace x=\left[x_1, x_2,...,x_D\right]|x_i>0,i=1,2,...,D;\sum_{i=1}^Dx_i=\kappa\rbrace
\]
 
 L'opération permettant de modifier la somme des composantes d'une composition (le \(\kappa\)) se nomme la closure:
  
 \[
C(x)=\left[\frac{\kappa*x_1}{\sum_{i=1}^Dx_i},\frac{\kappa*x_2}{\sum_{i=1}^Dx_i},...,\frac{\kappa*x_D}{\sum_{i=1}^Dx_i}     \right]
\]
Cette opération permet notamment de définir une sous-composition.
\vspace*{0.3cm}
Soit \(x\) une composition, une sous composition \(x_s\) de taille \(s\) est obtenue en appliquant la closure a un sous vecteur \(\left[x_{i1},x_{i2},...,x_{is}\right]\) de \(x\). 
\vspace*{0.2cm} \\
Deux vecteurs \(x,y\in\mathbb{R}^D_{+}\) tels que \(x_i,\ y_i>0,\ \forall i=1,...,D\) sont dits compositionnellement équivalents s'il existe un \(\lambda\in \mathbb{R}^{+}\) tel que \(x=\lambda y\) ce qui est équivalent à \(C(x)=C(y)\).


\subsection{Opération dans le simplexe}
Dans le simplexe, nous ne pouvons pas utiliser la géométrie euclidienne. Remarquons cela en prenant un exemple, soit 4 compositions:\\
\(
x_1=\left[0.1,0.4,0.5\right],\ x_2=\left[0.2,0.3,0.5\right],\ x_3=\left[0.4,0.4,0.2\right],\ x_4=\left[0.5,0.3,0.2\right]
\)
\\
La distance euclidienne entre la composition \(x_1\) et \(x_2\) est la même que celle entre \(x_3\) et \(x_4\), pourtant la proportion de la première composante a doublé dans le premier cas mais est loin d'avoir doublé dans le second.\\
Voici une des raisons pour laquelle nous avons besoin de définir une nouvelle géométrie.
\\
Perturbation d'une composition \(x\in S^D\) par une composition \(y\in S^D\),
\[
x\oplus y=C\left[x_1y_1,x_2y_2,...,x_Dy_D\right]
\]

La puissance d'une composition \(x\in S^D\) par une constante \(\alpha\in \mathbb{R}\), 
\[
x\in S^D,\ \alpha\in \mathbb{R},\ \alpha\odot =C\left[x_1^{\alpha},x_2^{\alpha},...,x_D^{\alpha}\right]
\]
Les deux opérations ci dessus permettent au simplexe de devenir un espace vectoriel (la perturbation en tant que loi de composition internet, la puissance en tant que loi de composition externe).\\
A cela, nous pouvons ajouter un produit scalaire ainsi qu'une distance et une norme associée.\\
Inner Product de deux composition \(x,y\in S^D\),
\[
x,y\in S^D,\ \left\langle x,y\right\rangle_a=\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^Dln
\frac{x_i}{x_j}ln\frac{y_i}{y_j}
\]

Distance entre \(x\) et \(y\in S^D\),
\[
x,y\in S^D,\ d_a\left(x,y\right)=\sqrt{\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^D\left(ln\frac{x_i}{x_j}-ln\frac{y_i}{y_j}\right)^2}
\]

Norme d'une \(x\in S^D\),
\[
\left\|x\right\|_a=\sqrt{\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^D\left(ln\frac{x_i}{x_j}\right)^2}
\]

\subsection{Transformation}
Comme on a pu le voir, l'espace du simplexe nécessite une redéfinition des opérations de base. Dans cette partie, nous allons voir des transformations permettant de repasser dans un espace euclidien.\\
Tout d'abord, remarquons que la valeur de \(\kappa\)(somme des composantes) d'une composition n'a pas fondamentalement une grande importance (def?). Les rapports relatifs sont bien plus intéressants. Il n'est donc pas étonnant que les transformations reposent sur des log ratios de composantes.\\

Commençons par l'additive log-ratio transformation(alr).
\[
alr:S^D\rightarrow R^{D-1},\ alr\left(x\right)=\left[ln\left(\frac{x_1}{x_D}\right), ln\left(\frac{x_2}{x_D}\right),...,ln\left(\frac{x_{D-1}}{x_D}\right)\right]
\]
La dernière composante d'une composition pouvant s'exprimer comme 
\[
x_D=\kappa -\sum_{i=1}^{D-1}x_i
\]
, le système sera alors lié. L'alr résout ce problème en prenant une composante comme dénominateur (la coordonnée est ainsi nulle).
Cependant, cette transformation est grandement dépendante de la composante choisie comme dénominateur. De plus, cette transformation ne préserve pas les distances.\\
\\
La transformation centred log-ratio n'a pas cette problématique.
Elle s'exprime comme suit:
\[
clr\left(x\right)=\left[ln\frac{x_1}{g(x)},...,ln\frac{x_D}{g(x)}\right]=\xi
\]	
avec \(g\) le centre de la composition définie par:
\[
 g(x)=\left(\prod^D_{i=1}x_i\right)^{1/D}
\]
Cependant, comme vu précédemment la dernière composante peut s'exprimer en fonction des autres, cette transformation conservant le nombre de coordonnées, les coordonnées ne seront pas indépendantes. En conséquence, la matrice de covariance est singulière. Notons que la somme des coordonnées est égale à 0.


Voyons enfin l'isometric log-ration transformation (ilr),
\[
ilr\left(x\right)=clr\left(x\right)\Psi^{'}
\]
avec \(\Psi\) l'image \(clr\) de la base.\\
Cette transformation permet de résoudre les problématiques des autres transformations, elle préserve les distances et ne génère pas une matrice de covariance liée. Pour cela, elle utilise un type de coordonnée particulier nommé balance.\\

Prenons comme système générateur, les vecteurs

\[
W_i=C\left(1,1,...,e,...\right),\quad i=1,...D-1
\]

en prenant l'image de ces vecteurs. Puis en utilisant Gram-Schmidt  pour l'orthogonaliser, on obtient la matrice \(\Psi\).\\
Exemple de matrice \(\Psi\) en dimension 5:

\[
\Psi =\begin{pmatrix}
\frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{-2}{\sqrt{5}}\\
\frac{1}{\sqrt{12}} & \frac{1}{\sqrt{12}} & \frac{1}{\sqrt{12}} & 
-\frac{\sqrt{3}}{\sqrt{4}} & 0 \\
\frac{1}{\sqrt{6}} & \frac{1}{\sqrt{6}} & -\frac{\sqrt{2}}{\sqrt{3}} & 0 & 0 \\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} & 0 & 0 & 0
\end{pmatrix}
\]

Si nous cherchons une interprétation des coordonnées obtenues par la transformation \(ilr\), nous pouvons regarder la matrice des séquences binaires associées à la base.\\
Si nous reprenons l'exemple, nous avons comme matrice binaire

\[
\begin{blockarray}{ccccc}
x1 & x2 & x3 & x4  & x5 \\
\begin{block}{(ccccc)}
1 & 1 & 1 & 1 & -1 \\
1 & 1 & 1 & -1 & 0 \\
1 & 1 & -1 & 0 & 0 \\
1 & -1 & 0 & 0 & 0 \\
\end{block}
\end{blockarray}
\]
La première coordonnée ilr représente ainsi le ratio des quatre premières composantes de la composition sur la cinquième.\\
La deuxième : les trois premières composantes sur la quatrième et ainsi de suite.\\
\\
Ces trois transformations ont bien entendue leur transformations inverse qui sont définie par:\\
\begin{align*}
&alr^{-1}\left(\xi\right)=C\left( \frac{\exp\left(\xi_1\right)}{\sum_{i=1}^{d-1} \exp\left(y_i\right) + 1},\ ...,\  \frac{1}{\sum_{i=1}^{d-1} \exp\left(y_i\right) + 1} \right)\\
&clr^{-1}\left(\xi\right)=C\left( \exp\left(\xi_1\right),\ ...,\  \exp\left(\xi_D\right)\ \right)\\
&ilr^{-1}=\left(x\right)=C\left(\exp{x\Psi}\right)
\end{align*}

\section{Analyse Multivariée}

\subsection{Réduction de dimension}
Dans notre cas, nous cherchons à appliquer un modèle probabiliste à nos données. L'ACP dans sa définition la plus commune ne permet pas de prendre en compte un tel modèle. Pour ce faire, nous allons donc utiliser une variante de l'ACP qui n'est autre que l'ACP probabiliste qui est définie comme:
\[
X_i=Wy_i + \mu + \epsilon_i
\]
où \(y_i\sim \mathcal{N}\left(0,I_d\right) \), \(\epsilon_i \sim \mathcal{N}\left(0,\ \sigma^2I_p\right)\) est un bruit gaussien et \(W\) une \(p \times d\) matrice.
\\
Si nous notons \(A\) la matrice des vecteurs propres de la matrice \(X^TX\), \(\Lambda\) la matrice diagonale contenant les valeurs propres associées et \(R\) une matrice orthogonale arbitraire, nous avons que le maximum de vraisemblance de \(W\) s'exprime comme:

\[
W_{ML}=A\left(\Lambda-\sigma^2I_d\right)^{1/2}R
\]
De même, l'estimateur du maximun de vraisemblance de \(\sigma^2\)
\[
\sigma^2_{ML}=\frac{1}{d-q}\sum_{j=q+1}^d \lambda_j
\]
Cet estimateur peut être perçu comme la variance perdue lors de la projection.\\
Pour la sélection de la dimension, nous avons tout simplement utilisé la méthode du coude.\\
\subsection{Mélange Gaussien}
\[
g\left(x; \pi,\theta\right)=\sum_{k=1}^K \pi_kf(x,\theta_k)
\]
où \(\theta=\left(\theta_1,...,\theta_k\right)\) sont les paramètres des lois gaussiennes et \( \pi=\left(\pi_1,...,\pi_K\right)\) sont les proportions du mélange Gaussien dont la somme est égale à 1.

\subsection{Schéma simulation}
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]
     \node[cloud] (X) {X};
     \node[cloud, below of=X] (S)      {\(S^p\)};
     \node[cloud, below of=S] (Y)      {Y};
     \node[block, below of=Y](apprentissage) {apprentissage};
     \node[cloud, below of=apprentissage] (Y_tilde) {\(\tilde{Y}\)};
     \node[cloud, below of=Y_tilde] (S1) {\(S^p\)};
     \node[block, below of=S1](ZIB) {Zero inflated};
     \node[cloud, below of=ZIB] (X_tilde) {\(\tilde{X}\)};
     
     
  %\draw[tuborg, decoration={brace}] let %\p1=(Y.north), \p2=(Y_tilde.south) in
%    ($(2, \y1)$) -- ($(2, \y2)$) node[tubnode] {Pro};
    
  \path[line] (X) --  node[midway, right] {MAP} (S);
  \path[line] (S) --  node[midway, right] {\(ilr\)} (Y);
  \path[line] (Y) --  node[midway, right] {} (apprentissage);
  \path[line] (apprentissage) --  node[midway, right] {} (Y_tilde);
  \path[line] (Y_tilde) --  node[midway, right] {\(ilr^{-1}\)} (S1);
  \path[line] (S1) --  node[midway, right] {multinomiale} (ZIB);
  \path[line] (ZIB) --  node[midway, right] {} (X_tilde);
 
  \end{tikzpicture}
\end{center}

\subsection{Apprentissage} 
\subsection{Maximum a posteriori}
Nos jeux de données étant des jeux de données de comptage, de nombreux zéros sont présents comme expliqué précédemment ; les transformations logarithmiques (ilr, clr) ne peuvent ainsi pas être directement appliquées.\\
Pour ce faire, nous n'allons pas utiliser l'estimateur du maximum de vraisemblance pour estimer la proportion de chaque OTU dans un échantillon.\\
En mettant une distribution a priori sur les observations \(X\) et en utilisant le théorème de Bayes, nous avons
 
 
\begin{align*}
P\left(\theta|X\right)&=\frac{P\left(X|\theta\right)P\left(\theta\right)}{P\left(X\right)} \\
&\propto P\left(X|\theta\right)P\left(\theta\right)
\end{align*}

Dans notre cas, nous allons prendre un a priori 
ce qui mène à l'estimateur suivant:
\[
MAP(p)=\underset{p}{argmax}\left(p|x\right)=\frac{x_i+1}{\sum_{i=1}^D\left(x_i+1\right)}
\]
 Cela revient simplement à ajouter un comptage pour chaque OTU.
\subsection{Zero inflated}
Les jeux de données contiennent un nombre excessif de zéros, la modélisation faite par un mélange gaussien ne parvient pas à bien prendre en compte tous ces zéros. Pour résoudre ce problème, nous allons modéliser cette particularité indépendamment. \\
Commençons par estimer la proportion de zéros pour chaque OTU de chaque gaussienne que nous notons \(\pi_{ik}\). Cette estimation est faite via le MAP comme pour la proportion d'un OTU dans un échantillon mais cette fois ci en prenant un pseudo comptage plus faible.\\
Par la suite, nous modélisons les zéros par une loi de Bernoulli,

\[P\left(Z=x\right)=\left\{
    \begin{array}{ll}
         \pi_{ik} & \text{ si } x=0 \\
         1 - \pi_{ik} & \text{ si } x=1 \\
    \end{array}
\right.
\]
Ce qui permet d'exprimer \(\tilde{X_{i}}\) comme 

\[
\tilde{X}_{i}=\left(Z_{1k},...,Z_{Dk}\right)*X_i
\]
où \(Z_{ik}\sim Bern\left(\pi_{ik}\right)\), \(X_{i}\sim  \mathcal{N}\left(\mu_k, \Sigma_k\right)\)
 
\section{Transformation ilr et classification supervise}
Mettons maintenant en pratique ces techniques.\\

\subsection*{Jeux de données}
Pour cela, cinq jeux de données sont mis à ma disposition qui sont Chaillou, Mach, Liver, Ravel ainsi que Vacher.\\

\subsubsection*{Chaillou}
Il s'agit d'un jeu de données d'origine alimentaire, il provient d'une étude sur les communautés bactériennes dans la nourriture périmée.\\
Pour cela, quatre types de viande : boeuf hache, veau hache, lardons, saucisses de volaille et quatre types de fruit de mer:saumon fumé, crevettes, filet de saumon et filet de cabillaud ont été échantillonnés.

\subsubsection*{Mach}
L'évolution du microbiote intestinal de porcelet au cours des premiers mois (avant, après le sevrage) ainsi que son impact sur le métabolisme était le sujet de l'étude contenant ce jeu de données.\\
Les excréments de 31 porcelets ont été séquencés à cinq périodes de leur vie (14, 36, 48, 60 et 70 jours) ; à chaque séquençage le sevrage ou non de l'animal a été relevé.

\subsubsection*{Liver}

\subsubsection*{Ravel}
Le microbiote vaginal permet-il de prévenir les maladies urogenital? C'était la question que se posaient les chercheurs dans l'étude contenant les données.\\
Pour répondre à cette question, ils ont séquencé le microbiote vaginal de 396 femmes provenant de quatre catégories ethniques différentes (Asiatiques, blanches, noires et hispanique). Sur chaque échantillon, le nugent score a été réalisé, il s'agit d'une méthode de diagnostic de la vaginose bactérienne.\\
De plus, durant cette étude, les échantillons ont été classés dans cinq clusters. 

\subsubsection*{Vacher}

\subsection*{Classification supervise}
Regardons maintenant les performances de classification supervisée (en faisant du leave one out) par le biais de trois algorithmes qui sont le random Forest, le k nearest neighbors et le support machine vector. \\
Pour montrer l'intérêt de passer par les données compositionnelles pour la classification, nous allons comparer les résultats d'une classification effectuée sur les données initiales (de comptage) et les données compositionnelles (après transformation ilr).\\

<<validation croise (leave one out), cache=TRUE>>=
v_chaillou <- validation_croise(chaillou, metadata_chaillou$EnvType)
v_ravel <- validation_croise(ravel, metadata_ravel$CST)
v_liver_500 <- validation_croise(liver_500, metadata_liver$status)
v_mach_500 <- validation_croise(mach_500, metadata_mach$Weaned)
@


<<validation croise donne ilr (leave one out), cache=TRUE>>=
v_chaillou_ilr <- validation_croise(chaillou %>% MAP() %>% ilr(), metadata_chaillou$EnvType)
v_ravel_ilr <- validation_croise(ravel %>% MAP() %>% ilr(), metadata_ravel$CST)
v_liver_500_ilr <- validation_croise(liver_500 %>% MAP() %>% ilr(), metadata_liver$status)
v_mach_500_ilr <- validation_croise(mach_500 %>% MAP() %>% ilr(), metadata_mach$Weaned)
@

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
Chaillou & \\
<<>>=
kable(v_chaillou$kNN)
@
&
<<>>=
kable(v_chaillou_ilr$kNN)
@
\end{tabular}
}

\section{Conclusion}
\end{document}