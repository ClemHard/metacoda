\documentclass[11pt,a4paper]{article}
\usepackage{tikz}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage[caption = false]{subfig}
\usepackage{bbm}
\usepackage[singlespacing]{setspace}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{textcomp}
\usepackage{apptools}
\usepackage{tabularx}
\usepackage{makecell} 
\usepackage{blkarray}
\usepackage{calc}
\usepackage{multirow}
\usepackage{array}
\usepackage{bigdelim}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes}

\tikzstyle{decision} = [diamond, draw, text width=4.5em, 
                        text badly centered, node distance=2cm, 
                        inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, 
                     text centered, rounded corners, 
                     minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, circle, minimum height=3em]

\newcommand{\ER}{\ensuremath{\mathbb{R}}}
\newcommand{\EE}{\ensuremath{\mathbb{E}}}

\tikzset{
>=stealth',
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

<<setup, include=FALSE, echo=FALSE>>=
library(knitr)
library(kableExtra)
library(xtable)
library(doParallel)
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, fig.path = "figure/", cache.path = "cache/")
knitr::opts_knit$set(root.dir = "..")
@

\title{Exploration des méthodes d'analyse de données compositionnelles pour l'étude du lien entre microbiote intestinal et santé.}
\author{Clément Hardy}
\date{}

\begin{document}

<<chargement_source, echo=FALSE,  include=FALSE>>=
source("Rscript/read_metagenomic_data.R")
source("Rscript/coda.R")
source("Rscript/comparaison_clustering.R")
source("Rscript/graph.R")
source("Rscript/bootstrap.R")
source("Rscript/test_bootstrap.R")
source("Rscript/tree_phyloseq.R")
source("Rscript/apprentissage_supervise.R")
@

<<clustering ravel, cache=TRUE>>=
k_ravel <- comparaison_k_means(ravel, metadata_ravel$CST, 5, 1)
hclust_ravel <- comparaison_hclust(ravel, metadata_ravel$CST, 5, 1)
Mclust_ravel <- comparaison_Mclust(ravel, metadata_ravel$CST, 5, 1)
@


<<clustering mach, cache=TRUE>>=
k_mach_500 <- comparaison_k_means(mach_500, metadata_mach$Weaned, 2, 1)
hclust_mach_500 <- comparaison_hclust(mach_500, metadata_mach$Weaned, 2, 1)
Mclust_mach_500 <- comparaison_Mclust(mach_500, metadata_mach$Weaned, 2, 1)
@

<<clustering chaillou, cache=TRUE>>=
k_chaillou <- comparaison_k_means(chaillou, metadata_chaillou$EnvType, 8, 1)
hclust_chaillou <- comparaison_hclust(chaillou, metadata_chaillou$EnvType, 8, 1)
Mclust_chaillou <- comparaison_Mclust(chaillou, metadata_chaillou$EnvType, 8, 1)
@


<<clustering vacher, cache=TRUE>>=
k_vacher <- comparaison_k_means(vacher, (metadata_vacher$pmInfection>0)*1, 2, 1)
hclust_vacher <- comparaison_hclust(vacher, (metadata_vacher$pmInfection>0)*1, 2, 1)
Mclust_vacher <- comparaison_Mclust(vacher, (metadata_vacher$pmInfection>0)*1, 2, 1)
@

<<clustering liver, cache=TRUE>>=
k_liver <- comparaison_k_means(liver_500, metadata_liver$status, 2, 1)
hclust_liver <- comparaison_hclust(liver_500, metadata_liver$status, 2, 1)
Mclust_liver <- comparaison_Mclust(liver_500, metadata_liver$status, 2, 1)
@


<<validation croise (leave one out), cache=TRUE>>=
v_chaillou <- validation_croise(chaillou, metadata_chaillou$EnvType)
v_ravel <- validation_croise(ravel, metadata_ravel$CST)
v_liver_500 <- validation_croise(liver_500, metadata_liver$status)
v_mach_500 <- validation_croise(mach_500, metadata_mach$Weaned)
@


<<validation croise donne ilr (leave one out), cache=TRUE>>=
v_chaillou_ilr <- validation_croise(chaillou %>% MAP() %>% ilr(), metadata_chaillou$EnvType)
v_ravel_ilr <- validation_croise(ravel %>% MAP() %>% ilr(), metadata_ravel$CST)
v_liver_500_ilr <- validation_croise(liver_500 %>% MAP() %>% ilr(), metadata_liver$status)
v_mach_500_ilr <- validation_croise(mach_500 %>% MAP() %>% ilr(), metadata_mach$Weaned)
@

<<bootstrap all donnee comptage, cache=TRUE>>=
t_chaillou <- test_bootstrap_all(chaillou, nb_cluster = 15, nb_axe = 16, nb_train = 10)
t_mach_500 <- test_bootstrap_all(mach_500, nb_cluster = 4, nb_axe = 10, nb_train = 10)
t_vacher <- test_bootstrap_all(vacher, nb_cluster = 4, nb_axe = 11, nb_train = 10)

t_liver_500 <- test_bootstrap_all(liver_500, nb_cluster = 7, nb_axe = 8, nb_train = 10)
t_ravel <- test_bootstrap_all(ravel, nb_cluster = 12, nb_axe = 12, nb_train = 10)
@

<<bootstrap supervise, cache=TRUE>>=
c_super <- test_bootstrap_supervise(chaillou, metadata_chaillou$EnvType, type="comptage", nb_train = 5)

r_super <- test_bootstrap_supervise(ravel, metadata_ravel$CST, type="comptage", nb_train = 5)

l_super <- test_bootstrap_supervise(liver_500, metadata_liver$status, type="comptage", nb_train = 5)

m_super <- test_bootstrap_supervise(mach_500, metadata_mach$Weaned, type="comptage", nb_train = 5)
@


<<bootstrap supervise ilr, cache=TRUE>>=
c_super_ilr <- test_bootstrap_supervise(chaillou, metadata_chaillou$EnvType, type="ilr", nb_train = 5)
r_super_ilr <- test_bootstrap_supervise(ravel, metadata_ravel$CST, type="ilr", nb_train = 5)
l_super_ilr <- test_bootstrap_supervise(liver_500, metadata_liver$status, type="ilr", nb_train = 5)
m_super_ilr <- test_bootstrap_supervise(mach_500, metadata_mach$Weaned, type="ilr", nb_train = 5)
@


<<classificateur supervise apres classificateur real_simu, cache=TRUE>>=
c_chaillou <- classificateur_group_real_simu(chaillou, metadata_chaillou$EnvType, nb_train = 5)
c_ravel <- classificateur_group_real_simu(ravel, metadata_ravel$CST, nb_train = 5)
c_liver <- classificateur_group_real_simu(liver_500, metadata_liver$status, nb_train = 5)
c_mach <- classificateur_group_real_simu(mach_500, metadata_mach$Weaned, nb_train = 5)
@



<<classificateur supervise apres classificateur real_simu ilr, cache=TRUE>>=
c_chaillou_ilr <- classificateur_group_real_simu(chaillou, metadata_chaillou$EnvType, type="ilr", nb_train = 5)
c_ravel_ilr <- classificateur_group_real_simu(ravel, metadata_ravel$CST, type="ilr", nb_train = 5)
c_liver_ilr <- classificateur_group_real_simu(liver_500, metadata_liver$status, type="ilr", nb_train = 5)
c_mach_ilr <- classificateur_group_real_simu(mach_500, metadata_mach$Weaned, type="ilr", nb_train = 5)
@


\maketitle

\tableofcontents


\section{Contexte biologique et enjeux}
\subsection{Séquençage}
\subsection{Données de comptage}
\subsection{Problème compositionnel}


\section{Géométrie de Aitchison}

\subsection{Simplexe}
Un vecteur \(x=\left[x_1,x_2,...x_D\right]\) est défini comme une D composition lorsque toutes ses composantes sont strictement positives et ne contiennent que de l'information relative.
\vspace*{0.3cm}. 
\\
Le simplexe est l'espace de probabilité des données compositionnelles  et est défini par: 
\[
    S^D=\lbrace x=\left[x_1, x_2,...,x_D\right]|x_i>0,i=1,2,...,D;\sum_{i=1}^Dx_i=\kappa\rbrace
\]
 
 L'opération permettant de modifier la somme des composantes d'une composition (le \(\kappa\)) se nomme la closure:
  
 \[
C(x)=\left[\frac{\kappa*x_1}{\sum_{i=1}^Dx_i},\frac{\kappa*x_2}{\sum_{i=1}^Dx_i},...,\frac{\kappa*x_D}{\sum_{i=1}^Dx_i}     \right]
\]
Cette opération permet notamment de définir une sous-composition.
\vspace*{0.3cm}
Soit \(x\) une composition, une sous composition \(x_s\) de taille \(s\) est obtenue en appliquant la closure a un sous vecteur \(\left[x_{i1},x_{i2},...,x_{is}\right]\) de \(x\). 
\vspace*{0.2cm} \\
Deux vecteurs \(x,y\in\mathbb{R}^D_{+}\) tels que \(x_i,\ y_i>0,\ \forall i=1,...,D\) sont dits compositionnellement équivalents s'il existe un \(\lambda\in \mathbb{R}^{+}\) tel que \(x=\lambda y\) ce qui est équivalent à \(C(x)=C(y)\).


\subsection{Opération dans le simplexe}
Dans le simplexe, nous ne pouvons pas utiliser la géométrie euclidienne. Remarquons cela en prenant un exemple, soit 4 compositions:\\
\(
x_1=\left[0.1,0.4,0.5\right],\ x_2=\left[0.2,0.3,0.5\right],\ x_3=\left[0.4,0.4,0.2\right],\ x_4=\left[0.5,0.3,0.2\right]
\)
\\
La distance euclidienne entre la composition \(x_1\) et \(x_2\) est la même que celle entre \(x_3\) et \(x_4\), pourtant la proportion de la première composante a doublé dans le premier cas mais est loin d'avoir doublé dans le second.\\
Voici une des raisons pour laquelle nous avons besoin de définir une nouvelle géométrie.
\\
Perturbation d'une composition \(x\in S^D\) par une composition \(y\in S^D\),
\[
x\oplus y=C\left[x_1y_1,x_2y_2,...,x_Dy_D\right]
\]

La puissance d'une composition \(x\in S^D\) par une constante \(\alpha\in \mathbb{R}\), 
\[
x\in S^D,\ \alpha\in \mathbb{R},\ \alpha\odot =C\left[x_1^{\alpha},x_2^{\alpha},...,x_D^{\alpha}\right]
\]
Les deux opérations ci dessus permettent au simplexe de devenir un espace vectoriel (la perturbation en tant que loi de composition internet, la puissance en tant que loi de composition externe).\\
A cela, nous pouvons ajouter un produit scalaire ainsi qu'une distance et une norme associée.\\
Inner Product de deux composition \(x,y\in S^D\),
\[
x,y\in S^D,\ \left\langle x,y\right\rangle_a=\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^Dln
\frac{x_i}{x_j}ln\frac{y_i}{y_j}
\]

Distance entre \(x\) et \(y\in S^D\),
\[
x,y\in S^D,\ d_a\left(x,y\right)=\sqrt{\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^D\left(ln\frac{x_i}{x_j}-ln\frac{y_i}{y_j}\right)^2}
\]

Norme d'une \(x\in S^D\),
\[
\left\|x\right\|_a=\sqrt{\frac{1}{2D}\sum_{i=1}^D\sum_{j=1}^D\left(ln\frac{x_i}{x_j}\right)^2}
\]

\subsection{Transformation}
Comme on a pu le voir, l'espace du simplexe nécessite une redéfinition des opérations de base. Dans cette partie, nous allons voir des transformations permettant de repasser dans un espace euclidien.\\
Tout d'abord, remarquons que la valeur de \(\kappa\)(somme des composantes) d'une composition n'a pas fondamentalement une grande importance (def?). Les rapports relatifs sont bien plus intéressants. Il n'est donc pas étonnant que les transformations reposent sur des log ratios de composantes.\\

Commençons par l'additive log-ratio transformation(alr).
\[
alr:S^D\rightarrow R^{D-1},\ alr\left(x\right)=\left[ln\left(\frac{x_1}{x_D}\right), ln\left(\frac{x_2}{x_D}\right),...,ln\left(\frac{x_{D-1}}{x_D}\right)\right]
\]
La dernière composante d'une composition pouvant s'exprimer comme 
\[
x_D=\kappa -\sum_{i=1}^{D-1}x_i
\]
, le système sera alors lié. L'alr résout ce problème en prenant une composante comme dénominateur (la coordonnée est ainsi nulle).
Cependant, cette transformation est grandement dépendante de la composante choisie comme dénominateur. De plus, cette transformation ne préserve pas les distances.\\
\\
La transformation centred log-ratio n'a pas cette problématique.
Elle s'exprime comme suit:
\[
clr\left(x\right)=\left[ln\frac{x_1}{g(x)},...,ln\frac{x_D}{g(x)}\right]=\xi
\]	
avec \(g\) le centre de la composition définie par:
\[
 g(x)=\left(\prod^D_{i=1}x_i\right)^{1/D}
\]
Cependant, comme vu précédemment la dernière composante peut s'exprimer en fonction des autres, cette transformation conservant le nombre de coordonnées, les coordonnées ne seront pas indépendantes. En conséquence, la matrice de covariance est singulière. Notons que la somme des coordonnées est égale à 0.


Voyons enfin l'isometric log-ration transformation (ilr),
\[
ilr\left(x\right)=clr\left(x\right)\Psi^{'}
\]
avec \(\Psi\) l'image \(clr\) de la base.\\
Cette transformation permet de résoudre les problématiques des autres transformations, elle préserve les distances et ne génère pas une matrice de covariance liée. Pour cela, elle utilise un type de coordonnée particulier nommé balance.\\

Prenons comme système générateur, les vecteurs

\[
W_i=C\left(1,1,...,e,...\right),\quad i=1,...D-1
\]

en prenant l'image \(clr\) de ces vecteurs. Puis en utilisant Gram-Schmidt  pour l'orthogonaliser, on obtient la matrice \(\Psi\).\\
Exemple de matrice \(\Psi\) en dimension 5:

\[
\Psi =\begin{pmatrix}
\frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{1}{\sqrt{20}} & \frac{-2}{\sqrt{5}}\\
\frac{1}{\sqrt{12}} & \frac{1}{\sqrt{12}} & \frac{1}{\sqrt{12}} & 
-\frac{\sqrt{3}}{\sqrt{4}} & 0 \\
\frac{1}{\sqrt{6}} & \frac{1}{\sqrt{6}} & -\frac{\sqrt{2}}{\sqrt{3}} & 0 & 0 \\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} & 0 & 0 & 0
\end{pmatrix}
\]

Si nous cherchons une interprétation des coordonnées obtenues par la transformation \(ilr\), nous pouvons regarder la matrice des séquences binaires associées à la base.\\
Si nous reprenons l'exemple, nous avons comme matrice binaire

\[
\begin{blockarray}{ccccc}
x1 & x2 & x3 & x4  & x5 \\
\begin{block}{(ccccc)}
1 & 1 & 1 & 1 & -1 \\
1 & 1 & 1 & -1 & 0 \\
1 & 1 & -1 & 0 & 0 \\
1 & -1 & 0 & 0 & 0 \\
\end{block}
\end{blockarray}
\]
La première coordonnée ilr représente ainsi le ratio des quatre premières composantes de la composition sur la cinquième.\\
La deuxième : les trois premières composantes sur la quatrième et ainsi de suite.\\
\\
Ces trois transformations ont bien entendue leur transformations inverse qui sont définie par:\\
\begin{align*}
&alr^{-1}\left(\xi\right)=C\left( \frac{\exp\left(\xi_1\right)}{\sum_{i=1}^{d-1} \exp\left(\xi_i\right) + 1},\ ...,\  \frac{1}{\sum_{i=1}^{d-1} \exp\left(\xi_i\right) + 1} \right)\\
&clr^{-1}\left(\xi\right)=C\left( \exp\left(\xi_1\right),\ ...,\  \exp\left(\xi_D\right)\ \right)\\
&ilr^{-1}\left(\xi\right)=C\left(\exp{\xi\Psi}\right)
\end{align*}

\section{Analyse Multivariée}

\subsection{Réduction de dimension}
Dans notre cas, nous cherchons à appliquer un modèle probabiliste à nos données. L'ACP dans sa définition la plus commune ne permet pas de prendre en compte un tel modèle. Pour ce faire, nous allons donc utiliser une variante de l'ACP qui n'est autre que l'ACP probabiliste qui est définie comme:
\[
X_i=Wy_i + \mu + \epsilon_i
\]
où \(y_i\sim \mathcal{N}\left(0,I_d\right) \), \(\epsilon_i \sim \mathcal{N}\left(0,\ \sigma^2I_p\right)\) est un bruit gaussien et \(W\) une \(p \times d\) matrice.
\\
Si nous notons \(A\) la matrice des vecteurs propres de la matrice \(X^TX\), \(\Lambda\) la matrice diagonale contenant les valeurs propres associées et \(R\) une matrice orthogonale arbitraire, nous avons que le maximum de vraisemblance de \(W\) s'exprime comme:

\[
W_{ML}=A\left(\Lambda-\sigma^2I_d\right)^{1/2}R
\]
De même, l'estimateur du maximun de vraisemblance de \(\sigma^2\)
\[
\sigma^2_{ML}=\frac{1}{d-q}\sum_{j=q+1}^d \lambda_j
\]
Cet estimateur peut être perçu comme la variance perdue lors de la projection.\\

\subsection{Maximum a posteriori}
Nos jeux de données étant des jeux de données de comptage, de nombreux zéros sont présents comme expliqué précédemment ; les transformations logarithmiques (ilr, clr) ne peuvent ainsi pas être directement appliquées.\\
Pour ce faire, nous n'allons pas utiliser l'estimateur du maximum de vraisemblance pour estimer la proportion de chaque OTU dans un échantillon.\\
En mettant une distribution a priori sur les observations \(X\) et en utilisant le théorème de Bayes, nous avons
 
 
\begin{align*}
P\left(\theta|X\right)&=\frac{P\left(X|\theta\right)P\left(\theta\right)}{P\left(X\right)} \\
&\propto P\left(X|\theta\right)P\left(\theta\right)
\end{align*}

Dans notre cas, nous allons prendre un a priori de Dirichlet sur les données,
ce qui mène à l'estimateur suivant:
\[
MAP(p)=\underset{p}{argmax}\left(p|x\right)=\frac{x_i+1}{\sum_{i=1}^D\left(x_i+1\right)}
\]
 Cela revient simplement à ajouter un comptage pour chaque OTU.

\section{Transformation ilr et classification}

\subsection{Jeux de données}
Pour cela, cinq jeux de données sont mis à ma disposition qui sont Chaillou, Mach, Liver, Ravel ainsi que Vacher.\\

\subsubsection*{Chaillou}
Il s'agit d'un jeu de données d'origine alimentaire, il provient d'une étude sur les communautés bactériennes dans la nourriture périmée.\\
Pour cela, quatre types de viande : boeuf hache, veau hache, lardons, saucisses de volaille et quatre types de fruit de mer:saumon fumé, crevettes, filet de saumon et filet de cabillaud ont été échantillonnés.

\subsubsection*{Mach}
L'évolution du microbiote intestinal de porcelet au cours des premiers mois (avant, après le sevrage) ainsi que son impact sur le métabolisme était le sujet de l'étude contenant ce jeu de données.\\
Les excréments de 31 porcelets ont été séquencés à cinq périodes de leur vie (14, 36, 48, 60 et 70 jours) ; à chaque séquençage le sevrage ou non de l'animal a été relevé.

\subsubsection*{Liver}

\subsubsection*{Ravel}
Le microbiote vaginal permet-il de prévenir les maladies urogenital? C'était la question que se posaient les chercheurs dans l'étude contenant les données.\\
Pour répondre à cette question, ils ont séquencé le microbiote vaginal de 396 femmes provenant de quatre catégories ethniques différentes (Asiatiques, blanches, noires et hispanique). Sur chaque échantillon, le nugent score a été réalisé, il s'agit d'une méthode de diagnostic de la vaginose bactérienne.\\
De plus, durant cette étude, les échantillons ont été classés dans cinq clusters. 

\subsubsection*{Vacher}

\subsection{Graphique ACP}
\subsection{Classification non supervise}
Dans chacun des jeux de données utilisés, les groupes semblent relativement bien séparé comme vu précédement (tout du moins sur les premiers axes d'une acp après transformation ilr).\\
Une des questions que nous pouvons alors nous poser est le passage aux données compositionnelles (puis transformation ilr) permet il de mieux distinguer les groupes?\\
Une classification non supervisé répond bien à cette question, ainsi nous allons utiliser plusieurs algorithmes (k-means, Mclust, classification hiérarchique) et regarder s'ils ont tendance à regrouper les échantillons des mêmes groupes.


\subsection{Classification supervise}
Regardons maintenant les performances de classification supervisée (en faisant du leave one out) par le biais de deux algorithmes qui sont, le k nearest neighbors et le support machine vector. \\
Pour montrer l'intérêt de passer par les données compositionnelles pour la classification, nous allons comparer les résultats d'une classification effectuée sur les données initiales (de comptage) et les données compositionnelles (après transformation \(ilr\)).\\

\begin{center}
Mach

\begin{tabular}{cc}
\multicolumn{2}{c}{kNN} \\
comptage & ilr \\
<<>>=
kable(v_mach_500$kNN)
@
&
<<>>=
kable(v_mach_500_ilr$kNN)
@
\end{tabular}


\begin{tabular}{cc}
\multicolumn{2}{c}{svm} \\
comptage & ilr \\
<<>>=
kable(v_mach_500$Svm)
@
&
<<>>=
kable(v_mach_500_ilr$Svm)
@
\end{tabular}
\end{center}
Nous remarquons que la classification supervise après transformation \(ilr\) permet d'améliorer sensiblement les résultats.\\
Dans le cas de certains classificateur tels que le svm, la classification supervise sur les données de comptage ne donne pas du tout des résultats concluant (voir Annexe \ref{annexe classification supervise}).

\section{Simulateur}

\subsection{Schéma simulateur}
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]
     \node[cloud] (X) {X};
     \node[cloud, below of=X] (S)      {S};
     \node[cloud, below of=S] (Y)      {Y};
     \node[block, below of=Y](apprentissage) {apprentissage};
     \node[cloud, below of=apprentissage] (Y_tilde) {\(\tilde{Y}\)};
     \node[cloud, below of=Y_tilde] (S1) {S};
     \node[block, below of=S1](ZIB) {Zero inflated};
     \node[cloud, below of=ZIB] (X_tilde) {\(\tilde{X}\)};
     
     
     
     \node[left of=apprentissage] (temp1_simulation) {};
     \node[left of=X_tilde] (temp2_simulation) {};
  %\draw[tuborg, decoration={brace}] let %\p1=(Y.north), \p2=(Y_tilde.south) in
%    ($(2, \y1)$) -- ($(2, \y2)$) node[tubnode] {Pro};
    
  \path[line] (X) --  node[midway, right] {MAP} (S);
  \path[line] (S) --  node[midway, right] {\(ilr\)} (Y);
  \path[line] (Y) --  node[midway, right] {} (apprentissage);
  \path[line] (apprentissage) --  node[midway, right] {} (Y_tilde);
  \path[line] (Y_tilde) --  node[midway, right] {\(ilr^{-1}\)} (S1);
  \path[line] (S1) --  node[midway, right] {multinomiale} (ZIB);
  \path[line] (ZIB) --  node[midway, right] {} (X_tilde);
  
\draw[decorate,decoration={brace,mirror}] (temp1_simulation.north west) -- node[midway, left] {Simulation} (temp2_simulation.south west);
  \end{tikzpicture}
\end{center}


La partie apprentissage sera divisé en cinq parties:
\begin{itemize}
\item apprentissage des zero (pour le zero inflated)
\item réduction de dimension
\item apprentissage de la densité (par mélange gaussien)
\item simulation de nouvelles données
\item ajout du bruit (le bruit de la PPCA)
\end{itemize}

\subsection{Mélange Gaussien}
L'apprentissage de la densité est confié à un mélange gaussien.\\
Un mélange gaussien est défini comme:
\[
g\left(x; \pi,\theta\right)=\sum_{k=1}^K \pi_kf(x,\theta_k)
\]
où \(\theta=\left(\theta_1,...,\theta_k\right)\) sont les paramètres des lois gaussiennes et \( \pi=\left(\pi_1,...,\pi_K\right)\) sont les proportions du mélange Gaussien dont la somme est égale à 1.


\subsection{Zero inflated}
Les jeux de données contiennent un nombre excessif de zéros, la modélisation faite par un mélange gaussien ne parvient pas à bien prendre en compte tous ces zéros. Pour résoudre ce problème, nous allons modéliser cette particularité indépendamment. \\
Commençons par estimer la proportion de zéros pour chaque OTU de chaque gaussienne que nous notons \(\pi_{ik}\). Cette estimation est faite via le MAP comme pour la proportion d'un OTU dans un échantillon mais cette fois ci en prenant un pseudo comptage plus faible.\\
Par la suite, nous modélisons les zéros par une loi de Bernoulli,

\[P\left(Z=x\right)=\left\{
    \begin{array}{ll}
         \pi_{ik} & \text{ si } x=0 \\
         1 - \pi_{ik} & \text{ si } x=1 \\
    \end{array}
\right.
\]
Ce qui permet d'exprimer \(\tilde{X_{i}}\) comme 

\[
\tilde{X}_{i}=\left(Z_{1k},...,Z_{Dk}\right)*X_i
\]
où \(Z_{ik}\sim Bern\left(\pi_{ik}\right)\), \(X_{i}\sim  \mathcal{N}\left(\mu_k, \Sigma_k\right)\)
 

\section{Performance simulateur}
Nous avons vue qu'il y a un certain intérêt à passer par une transformation \(ilr\) pour faire de la classification, nous allons tester la performance de notre simulateur.

\subsection{Méthode}
Nous allons simuler deux nouveaux jeux de données, l'un d'entre eux servira de jeux d'entrainement (avec les données réels) à un classificateur, l'autre jeu de données servira quant à lui de test. Une petite partie des données réels ne sont pas incluse dans le jeu d'entrainement, elles serviront elles aussi de test.\\
L'objectif d'une telle démarche est de regarder si un classificateur parvient à faire la différence entre les données réels et les données simulées.

\subsection{Résultats}
Dans notre cas, nous allons utilisé deux classificateurs, le random Forest ainsi que le k nearest neighbors (le classificateur svm a été ignoré pour sa faible efficacité pour les données de comptage).\\


\begin{center}
Mach

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(t_mach_500$all$random_forest)
@
&
<<>>=
kable(t_mach_500$all$kNN)
@
\end{tabular}
\end{center}

\subsection{Classification}
Passons à une autre méthode d'évaluation de l'efficacité du simulateur.\\
La simulation ne se fera plus sur l'ensemble du jeux de données mais séparément sur chaque groupe du jeu de donnée.\\
Les groupes des données simulées seront alors connu et nous pourrons vérifier si le classificateur retrouve bien ses groupes.\\
Voici un schéma illustratif de la méthode avec un jeu de données de deux groupes  (dans le cas de mach: sevre, non sevre).
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]
     \node[cloud] (X1) {X1};
     \node[right of=X1, node distance=1cm] (temp) {};
     \node[cloud, right of=temp, node distance=1cm] (X2)      {X2};
     \node[cloud, below of=X2] (X2_tilde)   {\(\tilde{X2}\)};
     \node[cloud, below of=X1] (X1_tilde)   {\(\tilde{X1}\)};
     \node[below of=temp] (temp2) {};
     \node[cloud, below of=temp2](X_tilde) {\(\tilde{X}\)};
     \node[below of=X_tilde] (temp3) {};
     \node[cloud, left of=temp3, node distance=1cm] (X3_tilde) {\(\tilde{X3}\)};
     \node[cloud, right of=temp3, node distance=1cm] (X4_tilde) {\(\tilde{X4}\)};
     
    % \draw[tuborg, decoration={brace}] let \p1=(X2.south), %\p2=(X2_tilde.north) in
  %  ($(2, \y1)$) -- ($(2, \y2)$) node[tubnode] {Pro};
     
   	\path[line] (X1) --  (X1_tilde);
  	\path[line] (X2) --  node[midway, right] {Apprentissage, simulation}(X2_tilde);
 	\path[line] (X1_tilde) --  (X_tilde);
 	\path[line] (X2_tilde) --  (X_tilde);
 	\path[line] (X_tilde) --  (X3_tilde);
 	\path[line] (X_tilde) --  node[midway, right] {Classification}(X4_tilde);
 	
  \end{tikzpicture}
\end{center}
Une bonne simulation devrait résulter en la présence des mêmes échantillons dans \(\tilde{X1}\) et \(\tilde{X3}\), de même que dans \(\tilde{X2}\) et \(\tilde{X4}\).\\


\begin{center}
Mach

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(m_super$all$random_forest)
@
&
<<>>=
kable(m_super$all$kNN)
@
\end{tabular}
\end{center}

Comme nous avons pu le voir ci dessus, une grande partie des échantillons simulés est malheureusement "mal simulé" (notamment dans le cas du jeu de données liver). Pour vérifier que les données considéré comme bien "simulées" sont facilement classable dans un groupe (et ainsi relativement proche des données réels de leur groupe respectif), nous allons conserver uniquement ces données puis faire comme précédement en faisant une classification supervisée puis en vérifiant leur bonne classification.

Voici un schéma récapitulatif de la méthode.
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]
     \node[cloud] (X1) {X1};
     \node[right of=X1, node distance=1cm] (temp) {};
     \node[cloud, right of=temp, node distance=1cm] (X2)      {X2};
     \node[cloud, below of=X2] (X2_tilde)   {\(\tilde{X2}\)};
     \node[cloud, below of=X1] (X1_tilde)   {\(\tilde{X1}\)};
     \node[below of=temp] (temp2) {};
     \node[cloud, below of=temp2](X_2_tilde) {\(\tilde{X}_2\)};
     \node[below of=X_tilde, node distance=1cm] (temp3) {};
     \node[cloud, below of=X_2_tilde](X_real_tilde) {\(\tilde{X}_{real}\)};
     \node[below of=X_real_tilde] (temp4) {};
     \node[cloud, left of=temp4, node distance=1cm] (X3_tilde) {\(\tilde{X3}\)};
     \node[cloud, right of=temp4, node distance=1cm] (X4_tilde) {\(\tilde{X4}\)};
     
     
     
     \node[block, right of=X_tilde, node distance=6cm, text width=3cm] (Apprentissage classificateur) {Apprentissage classificateur real/simu};
     \node[cloud, above of=Apprentissage classificateur] (X_1_tilde) {\(\tilde{X}_1\)}; 
     \node[cloud, above of=X_1_tilde] (X) {X};
     
     
     
     
   	\path[line] (X1) --  node[midway, left] {Apprentissage, simulation}(X1_tilde);
  	\path[line] (X2) --  (X2_tilde);
 	\path[line] (X1_tilde) --  (X_tilde);
 	\path[line] (X2_tilde) --  (X_tilde);
 	\path[line] (X_tilde) -- node[midway, left] {Classification real/simu} (X_real_tilde);
 	\path[line] (X_real_tilde) --  node[midway, left] {Classification} (X3_tilde);
 	\path[line] (X_real_tilde) --  (X4_tilde);
 	\path[line] (Apprentissage classificateur) |- (temp3);
 	\path[line] (X) -- (X_1_tilde);
 	\path[line] (X_1_tilde) -- (Apprentissage classificateur);
 	
  \end{tikzpicture}
\end{center}



\begin{center}
Mach

\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_mach$all$random_forest)
@
&
<<>>=
kable(c_mach$all$kNN)
@
\end{tabular}
\end{center}


\section{Conclusion}

\section{Annexe}
\appendix
\subsection{classification non supervise}\label{annexe classification non supervise}
\subsection{classification supervise}\label{annexe classification supervise}
\begin{center}
Chaillou
\end{center}


\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{kNN} \\
comptage & ilr \\
<<>>=
kable(v_chaillou$kNN)
@
&
<<>>=
kable(v_chaillou_ilr$kNN)
@
\end{tabular}
}
\end{center}


\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\multicolumn{2}{c}{svm} \\
comptage & ilr \\
<<>>=
kable(v_chaillou$Svm)
@
&
<<>>=
kable(v_chaillou_ilr$Svm)
@
\end{tabular}
}
\end{center}



\subsection{Performance simulateur}
\subsubsection*{Sur le jeu de données entier}\label{annexe donnees entier}
\begin{center}
Chaillou
\end{center}

\begin{center}
\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(t_chaillou$all$random_forest)
@
&
<<>>=
kable(t_chaillou$all$kNN)
@
\end{tabular}
\end{center}

\subsubsection*{Sur les groupes}\label{annexe sur les groupes}
\begin{center}
Chaillou

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_super$all$random_forest)
@
&
<<>>=
kable(c_super$all$kNN)
@
\end{tabular}
}
\end{center}


\subsubsection*{Sur les données simulées considéré comme réels}\label{reels/simu}
\begin{center}
Chaillou

\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
Random forest & kNN \\
<<>>=
kable(c_chaillou$all$random_forest)
@
&
<<>>=
kable(c_chaillou$all$kNN)
@
\end{tabular}
}
\end{center}
\end{document}